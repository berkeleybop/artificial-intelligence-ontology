<?xml version="1.0"?>
<rdf:RDF xmlns="http://purl.obolibrary.org/obo/aio.owl#"
     xml:base="http://purl.obolibrary.org/obo/aio.owl"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:obo="http://purl.obolibrary.org/obo/"
     xmlns:owl="http://www.w3.org/2002/07/owl#"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
     xmlns:xml="http://www.w3.org/XML/1998/namespace"
     xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
     xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
     xmlns:terms="http://purl.org/dc/terms/"
     xmlns:oboInOwl="http://www.geneontology.org/formats/oboInOwl#">
    <owl:Ontology rdf:about="http://purl.obolibrary.org/obo/aio.owl">
        <owl:versionIRI rdf:resource="http://purl.obolibrary.org/obo/aio/releases/2023-09-08/aio.owl"/>
        <dc:description>This ontology models classes and relationships describing deep learning networks, their component layers and activation functions, as well as potential biases.</dc:description>
        <dc:title>Artificial Intelligence Ontology</dc:title>
        <terms:license rdf:resource="http://creativecommons.org/licenses/by/4.0/"/>
        <owl:versionInfo>2023-09-08</owl:versionInfo>
    </owl:Ontology>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Annotation properties
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://purl.obolibrary.org/obo/IAO_0000115 -->

    <owl:AnnotationProperty rdf:about="http://purl.obolibrary.org/obo/IAO_0000115"/>
    


    <!-- http://purl.org/dc/elements/1.1/description -->

    <owl:AnnotationProperty rdf:about="http://purl.org/dc/elements/1.1/description"/>
    


    <!-- http://purl.org/dc/elements/1.1/title -->

    <owl:AnnotationProperty rdf:about="http://purl.org/dc/elements/1.1/title"/>
    


    <!-- http://purl.org/dc/terms/license -->

    <owl:AnnotationProperty rdf:about="http://purl.org/dc/terms/license"/>
    


    <!-- http://www.geneontology.org/formats/oboInOwl#hasDbXref -->

    <owl:AnnotationProperty rdf:about="http://www.geneontology.org/formats/oboInOwl#hasDbXref"/>
    


    <!-- http://www.geneontology.org/formats/oboInOwl#hasExactSynonym -->

    <owl:AnnotationProperty rdf:about="http://www.geneontology.org/formats/oboInOwl#hasExactSynonym"/>
    


    <!-- http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym -->

    <owl:AnnotationProperty rdf:about="http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Classes
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- https://w3id.org/aio/AE -->

    <owl:Class rdf:about="https://w3id.org/aio/AE"/>
    


    <!-- https://w3id.org/aio/ANN -->

    <owl:Class rdf:about="https://w3id.org/aio/ANN"/>
    


    <!-- https://w3id.org/aio/AbstractRNNCell -->

    <owl:Class rdf:about="https://w3id.org/aio/AbstractRNNCell">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Abstract object representing an RNN cell. This is the base class for implementing RNN cells with custom behavior.</obo:IAO_0000115>
        <rdfs:label>AbstractRNNCell</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AbstractRNNCell"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Abstract object representing an RNN cell. This is the base class for implementing RNN cells with custom behavior.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/AbstractRNNCell</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Activation_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Activation_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Applies an activation function to an output.</obo:IAO_0000115>
        <rdfs:label>Activation Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Activation_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies an activation function to an output.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Active_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Active_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Methods which can interactively query a user (or some other information source) to label new data points with the desired outputs.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Query Learning</oboInOwl:hasExactSynonym>
        <rdfs:label>Active Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Active_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods which can interactively query a user (or some other information source) to label new data points with the desired outputs.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Active_learning_(machine_learning)</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/ActivityRegularization_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/ActivityRegularization_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regularization_Layer"/>
        <obo:IAO_0000115>Layer that applies an update to the cost function based input activity.</obo:IAO_0000115>
        <rdfs:label>ActivityRegularization Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/ActivityRegularization_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer that applies an update to the cost function based input activity.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/ActivityRegularization</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Activity_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Activity_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Use_And_Interpretation_Bias"/>
        <obo:IAO_0000115>A type of selection bias that occurs when systems/platforms get their training data from their most active users, rather than those less active (or inactive).</obo:IAO_0000115>
        <rdfs:label>Activity Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Activity_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A type of selection bias that occurs when systems/platforms get their training data from their most active users, rather than those less active (or inactive).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AdaptiveAvgPool1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AdaptiveAvgPool1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AdaptiveAvgPool1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>AdaptiveAvgPool1d</oboInOwl:hasExactSynonym>
        <rdfs:label>AdaptiveAvgPool1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AdaptiveAvgPool1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AdaptiveAvgPool2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AdaptiveAvgPool2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AdaptiveAvgPool2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>AdaptiveAvgPool2d</oboInOwl:hasExactSynonym>
        <rdfs:label>AdaptiveAvgPool2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AdaptiveAvgPool2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AdaptiveAvgPool3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AdaptiveAvgPool3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AdaptiveAvgPool3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>AdaptiveAvgPool3d</oboInOwl:hasExactSynonym>
        <rdfs:label>AdaptiveAvgPool3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AdaptiveAvgPool3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AdaptiveMaxPool1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AdaptiveMaxPool1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AdaptiveMaxPool1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>AdaptiveMaxPool1d</oboInOwl:hasExactSynonym>
        <rdfs:label>AdaptiveMaxPool1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AdaptiveMaxPool1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AdaptiveMaxPool2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AdaptiveMaxPool2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AdaptiveMaxPool2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>AdaptiveMaxPool2d</oboInOwl:hasExactSynonym>
        <rdfs:label>AdaptiveMaxPool2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AdaptiveMaxPool2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AdaptiveMaxPool3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AdaptiveMaxPool3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AdaptiveMaxPool3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>AdaptiveMaxPool3d</oboInOwl:hasExactSynonym>
        <rdfs:label>AdaptiveMaxPool3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AdaptiveMaxPool3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Add_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Add_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Merging_Layer"/>
        <obo:IAO_0000115>Layer that adds a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</obo:IAO_0000115>
        <rdfs:label>Add Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Add_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer that adds a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AdditiveAttention_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AdditiveAttention_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Attention_Layer"/>
        <obo:IAO_0000115>Additive attention layer, a.k.a. Bahdanau-style attention.</obo:IAO_0000115>
        <rdfs:label>AdditiveAttention Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AdditiveAttention_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Additive attention layer, a.k.a. Bahdanau-style attention.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AlphaDropout_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AlphaDropout_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regularization_Layer"/>
        <obo:IAO_0000115>Applies Alpha Dropout to the input. Alpha Dropout is a Dropout that keeps mean and variance of inputs to their original values, in order to ensure the self-normalizing property even after this dropout. Alpha Dropout fits well to Scaled Exponential Linear Units by randomly setting activations to the negative saturation value.</obo:IAO_0000115>
        <rdfs:label>AlphaDropout Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AlphaDropout_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies Alpha Dropout to the input. Alpha Dropout is a Dropout that keeps mean and variance of inputs to their original values, in order to ensure the self-normalizing property even after this dropout. Alpha Dropout fits well to Scaled Exponential Linear Units by randomly setting activations to the negative saturation value.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/AlphaDropout</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Amplification_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Amplification_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Processing_Bias"/>
        <obo:IAO_0000115>Arises when the distribution over prediction outputs is skewed in comparison to the prior distribution of the prediction target.</obo:IAO_0000115>
        <rdfs:label>Amplification Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Amplification_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises when the distribution over prediction outputs is skewed in comparison to the prior distribution of the prediction target.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Anchoring_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Anchoring_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>A cognitive bias, the influence of a particular reference point or anchor on people’s decisions. Often more fully referred to as anchoring-and-adjustment, or anchoring-and-adjusting: after an anchor is set, people adjust insufficiently from that anchor point to arrive at a final answer. Decision makers are biased towards an initially presented value.</obo:IAO_0000115>
        <rdfs:label>Anchoring Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Anchoring_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A cognitive bias, the influence of a particular reference point or anchor on people’s decisions. Often more fully referred to as anchoring-and-adjustment, or anchoring-and-adjusting: after an anchor is set, people adjust insufficiently from that anchor point to arrive at a final answer. Decision makers are biased towards an initially presented value.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Annotator_Reporting_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Annotator_Reporting_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>When users rely on automation as a heuristic replacement for their own information seeking and processing. A form of individual bias but often discussed as a group bias, or the larger effects on natural language processing models.</obo:IAO_0000115>
        <rdfs:label>Annotator Reporting Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Annotator_Reporting_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>When users rely on automation as a heuristic replacement for their own information seeking and processing. A form of individual bias but often discussed as a group bias, or the larger effects on natural language processing models.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Artificial_Neural_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Artificial_Neural_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Network"/>
        <obo:IAO_0000115>An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives a signal then processes it and can signal neurons connected to it. The &quot;signal&quot; at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as Learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>ANN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>NN</oboInOwl:hasExactSynonym>
        <rdfs:label>Artificial Neural Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Artificial_Neural_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives a signal then processes it and can signal neurons connected to it. The &quot;signal&quot; at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as Learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Artificial_neural_network</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Association_Rule_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Association_Rule_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Supervised_Learning"/>
        <obo:IAO_0000115>A rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.</obo:IAO_0000115>
        <rdfs:label>Association Rule Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Association_Rule_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Association_rule_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Attention_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Attention_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Dot-product attention layer, a.k.a. Luong-style attention.</obo:IAO_0000115>
        <rdfs:label>Attention Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Attention_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Dot-product attention layer, a.k.a. Luong-style attention.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Auto_Encoder_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Auto_Encoder_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/UPN"/>
        <obo:IAO_0000115>An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised Learning). The encoding is validated and refined by attempting to regenerate the input from the encoding. The autoencoder learns a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore insignificant data (“noise”). (https://en.wikipedia.org/wiki/Autoencoder)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AE</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Hidden, Matched Output-Input</rdfs:comment>
        <rdfs:label>Auto Encoder Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Auto_Encoder_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised Learning). The encoding is validated and refined by attempting to regenerate the input from the encoding. The autoencoder learns a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore insignificant data (“noise”). (https://en.wikipedia.org/wiki/Autoencoder)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Autoencoder</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Automation_Complacency_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Automation_Complacency_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>When humans over-rely on automated systems or have their skills attenuated by such over-reliance (e.g., spelling and autocorrect or spellcheckers).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Automation Complaceny</oboInOwl:hasExactSynonym>
        <rdfs:label>Automation Complacency Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Automation_Complacency_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>When humans over-rely on automated systems or have their skills attenuated by such over-reliance (e.g., spelling and autocorrect or spellcheckers).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Availability_Heuristic_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Availability_Heuristic_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>A mental shortcut whereby people tend to overweight what comes easily or quickly to mind, meaning that what is easier to recall—e.g., more “available”—receives greater emphasis in judgement and decision-making.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Availability Bias</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Availability Heuristic</oboInOwl:hasExactSynonym>
        <rdfs:label>Availability Heuristic Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Availability_Heuristic_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A mental shortcut whereby people tend to overweight what comes easily or quickly to mind, meaning that what is easier to recall—e.g., more “available”—receives greater emphasis in judgement and decision-making.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AveragePooling1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AveragePooling1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Average pooling for temporal data. Downsamples the input representation by taking the average value over the window defined by pool_size. The window is shifted by strides. The resulting output when using &quot;valid&quot; padding option has a shape of: output_shape = (input_shape - pool_size + 1) / strides). The resulting output shape when using the &quot;same&quot; padding option is: output_shape = input_shape / strides.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AvgPool1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>AvgPool1d</oboInOwl:hasExactSynonym>
        <rdfs:label>AveragePooling1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AveragePooling1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Average pooling for temporal data. Downsamples the input representation by taking the average value over the window defined by pool_size. The window is shifted by strides. The resulting output when using &quot;valid&quot; padding option has a shape of: output_shape = (input_shape - pool_size + 1) / strides). The resulting output shape when using the &quot;same&quot; padding option is: output_shape = input_shape / strides.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AveragePooling2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AveragePooling2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Average pooling operation for spatial data. Downsamples the input along its spatial dimensions (height and width) by taking the average value over an input window (of size defined by pool_size) for each channel of the input. The window is shifted by strides along each dimension. The resulting output when using &quot;valid&quot; padding option has a shape (number of rows or columns) of: output_shape = math.floor((input_shape - pool_size) / strides) + 1 (when input_shape &gt;= pool_size). The resulting output shape when using the &quot;same&quot; padding option is: output_shape = math.floor((input_shape - 1) / strides) + 1.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AvgPool2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>AvgPool2d</oboInOwl:hasExactSynonym>
        <rdfs:label>AveragePooling2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AveragePooling2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Average pooling operation for spatial data. Downsamples the input along its spatial dimensions (height and width) by taking the average value over an input window (of size defined by pool_size) for each channel of the input. The window is shifted by strides along each dimension. The resulting output when using &quot;valid&quot; padding option has a shape (number of rows or columns) of: output_shape = math.floor((input_shape - pool_size) / strides) + 1 (when input_shape &gt;= pool_size). The resulting output shape when using the &quot;same&quot; padding option is: output_shape = math.floor((input_shape - 1) / strides) + 1.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AveragePooling3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AveragePooling3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Average pooling operation for 3D data (spatial or spatio-temporal). Downsamples the input along its spatial dimensions (depth, height, and width) by taking the average value over an input window (of size defined by pool_size) for each channel of the input. The window is shifted by strides along each dimension.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AvgPool3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>AvgPool3d</oboInOwl:hasExactSynonym>
        <rdfs:label>AveragePooling3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AveragePooling3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Average pooling operation for 3D data (spatial or spatio-temporal). Downsamples the input along its spatial dimensions (depth, height, and width) by taking the average value over an input window (of size defined by pool_size) for each channel of the input. The window is shifted by strides along each dimension.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling3D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Average_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Average_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Merging_Layer"/>
        <obo:IAO_0000115>Layer that averages a list of inputs element-wise. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</obo:IAO_0000115>
        <rdfs:label>Average Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Average_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer that averages a list of inputs element-wise. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Average</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AvgPool1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AvgPool1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 1D average pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AvgPool1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>AvgPool1d</oboInOwl:hasExactSynonym>
        <rdfs:label>AvgPool1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AvgPool1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 1D average pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AvgPool2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AvgPool2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 2D average pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AvgPool2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>AvgPool2d</oboInOwl:hasExactSynonym>
        <rdfs:label>AvgPool2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AvgPool2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 2D average pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/AvgPool3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/AvgPool3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 3D average pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>AvgPool3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>AvgPool3d</oboInOwl:hasExactSynonym>
        <rdfs:label>AvgPool3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/AvgPool3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 3D average pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/BM -->

    <owl:Class rdf:about="https://w3id.org/aio/BM"/>
    


    <!-- https://w3id.org/aio/BatchNorm1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/BatchNorm1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/BatchNormalization_Layer"/>
        <obo:IAO_0000115>Applies Batch Normalization over a 2D or 3D input as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>BatchNorm1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>BatchNorm1d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.BatchNorm1d</oboInOwl:hasExactSynonym>
        <rdfs:label>BatchNorm1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/BatchNorm1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies Batch Normalization over a 2D or 3D input as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/BatchNorm2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/BatchNorm2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/BatchNormalization_Layer"/>
        <obo:IAO_0000115>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>BatchNorm2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>BatchNorm2d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.BatchNorm2d</oboInOwl:hasExactSynonym>
        <rdfs:label>BatchNorm2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/BatchNorm2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/BatchNorm3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/BatchNorm3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/BatchNormalization_Layer"/>
        <obo:IAO_0000115>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>BatchNorm3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>BatchNorm3d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.BatchNorm3d</oboInOwl:hasExactSynonym>
        <rdfs:label>BatchNorm3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/BatchNorm3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/BatchNormalization_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/BatchNormalization_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Normalization_Layer"/>
        <obo:IAO_0000115>Layer that normalizes its inputs. Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. Importantly, batch normalization works differently during training and during inference. During training (i.e. when using fit() or when calling the layer/model with the argument training=True), the layer normalizes its output using the mean and standard deviation of the current batch of inputs. That is to say, for each channel being normalized, the layer returns gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta, where: epsilon is small constant (configurable as part of the constructor arguments), gamma is a learned scaling factor (initialized as 1), which can be disabled by passing scale=False to the constructor. beta is a learned offset factor (initialized as 0), which can be disabled by passing center=False to the constructor. During inference (i.e. when using evaluate() or predict() or when calling the layer/model with the argument training=False (which is the default), the layer normalizes its output using a moving average of the mean and standard deviation of the batches it has seen during training. That is to say, it returns gamma * (batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta. self.moving_mean and self.moving_var are non-trainable variables that are updated each time the layer in called in training mode, as such: moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum) moving_var = moving_var * momentum + var(batch) * (1 - momentum).</obo:IAO_0000115>
        <rdfs:label>BatchNormalization Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/BatchNormalization_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer that normalizes its inputs. Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. Importantly, batch normalization works differently during training and during inference. During training (i.e. when using fit() or when calling the layer/model with the argument training=True), the layer normalizes its output using the mean and standard deviation of the current batch of inputs. That is to say, for each channel being normalized, the layer returns gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta, where: epsilon is small constant (configurable as part of the constructor arguments), gamma is a learned scaling factor (initialized as 1), which can be disabled by passing scale=False to the constructor. beta is a learned offset factor (initialized as 0), which can be disabled by passing center=False to the constructor. During inference (i.e. when using evaluate() or predict() or when calling the layer/model with the argument training=False (which is the default), the layer normalizes its output using a moving average of the mean and standard deviation of the batches it has seen during training. That is to say, it returns gamma * (batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta. self.moving_mean and self.moving_var are non-trainable variables that are updated each time the layer in called in training mode, as such: moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum) moving_var = moving_var * momentum + var(batch) * (1 - momentum).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Bayesian_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Bayesian_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Network"/>
        <obo:IAO_0000115>A probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG).</obo:IAO_0000115>
        <rdfs:label>Bayesian Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Bayesian_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Bayesian_network</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Behavioral_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Behavioral_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>Systematic distortions in user behavior across platforms or contexts, or across users represented in different datasets.</obo:IAO_0000115>
        <rdfs:label>Behavioral Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Behavioral_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Systematic distortions in user behavior across platforms or contexts, or across users represented in different datasets.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Bias">
        <obo:IAO_0000115>Systematic error introduced into sampling or testing by selecting or encouraging one outcome or answer over others.</obo:IAO_0000115>
        <rdfs:label>Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Systematic error introduced into sampling or testing by selecting or encouraging one outcome or answer over others.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.merriam-webster.com/dictionary/bias</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Biclustering -->

    <owl:Class rdf:about="https://w3id.org/aio/Biclustering">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Methods that simultaneously cluster the rows and columns of a matrix.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Block Clustering</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Co-clustering</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Joint Clustering</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Two-mode Clustering</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Two-way Clustering</oboInOwl:hasExactSynonym>
        <rdfs:label>Biclustering</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Biclustering"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that simultaneously cluster the rows and columns of a matrix.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Biclustering</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Bidirectional_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Bidirectional_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Recurrent_Layer"/>
        <obo:IAO_0000115>Bidirectional wrapper for RNNs.</obo:IAO_0000115>
        <rdfs:label>Bidirectional Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Bidirectional_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Bidirectional wrapper for RNNs.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Binary_Classification -->

    <owl:Class rdf:about="https://w3id.org/aio/Binary_Classification">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Classification"/>
        <obo:IAO_0000115>Methods that classify the elements of a set into two groups (each called class) on the basis of a classification rule.</obo:IAO_0000115>
        <rdfs:label>Binary Classification</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Binary_Classification"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that classify the elements of a set into two groups (each called class) on the basis of a classification rule.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Binary_classification</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Boltzmann_Machine_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Boltzmann_Machine_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/SCN"/>
        <obo:IAO_0000115>A Boltzmann machine is a type of stochastic recurrent neural network. It is a Markov random field. It was translated from statistical physics for use in cognitive science. The Boltzmann machine is based on a stochastic spin-glass model with an external field, i.e., a Sherrington–Kirkpatrick model that is a stochastic Ising Model[2] and applied to machine Learning.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>BM</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Sherrington–Kirkpatrick model with external field</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>stochastic Hopfield network with hidden units</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>stochastic Ising-Lenz-Little model</oboInOwl:hasExactSynonym>
        <rdfs:comment>Backfed Input, Probabilistic Hidden</rdfs:comment>
        <rdfs:label>Boltzmann Machine Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Boltzmann_Machine_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A Boltzmann machine is a type of stochastic recurrent neural network. It is a Markov random field. It was translated from statistical physics for use in cognitive science. The Boltzmann machine is based on a stochastic spin-glass model with an external field, i.e., a Sherrington–Kirkpatrick model that is a stochastic Ising Model[2] and applied to machine Learning.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Boltzmann_machine</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Categorical_Features_Preprocessing_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Categorical_Features_Preprocessing_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A layer that performs categorical data preprocessing operations.</obo:IAO_0000115>
        <rdfs:label>Categorical Features Preprocessing Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Categorical_Features_Preprocessing_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A layer that performs categorical data preprocessing operations.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://keras.io/guides/preprocessing_layers/</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/CategoryEncoding_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/CategoryEncoding_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Categorical_Features_Preprocessing_Layer"/>
        <obo:IAO_0000115>A preprocessing layer which encodes integer features. This layer provides options for condensing data into a categorical encoding when the total number of tokens are known in advance. It accepts integer values as inputs, and it outputs a dense or sparse representation of those inputs. For integer inputs where the total number of tokens is not known, use tf.keras.layers.IntegerLookup instead.</obo:IAO_0000115>
        <rdfs:label>CategoryEncoding Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/CategoryEncoding_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which encodes integer features. This layer provides options for condensing data into a categorical encoding when the total number of tokens are known in advance. It accepts integer values as inputs, and it outputs a dense or sparse representation of those inputs. For integer inputs where the total number of tokens is not known, use tf.keras.layers.IntegerLookup instead.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/CategoryEncoding</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Causal_Graphical_Model -->

    <owl:Class rdf:about="https://w3id.org/aio/Causal_Graphical_Model">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Probabilistic_Graphical_Model"/>
        <obo:IAO_0000115>Probabilistic graphical models used to encode assumptions about the data-generating process.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Casaul Bayesian Network</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Casaul Graph</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>DAG</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Directed Acyclic Graph</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Path Diagram</oboInOwl:hasExactSynonym>
        <rdfs:label>Causal Graphical Model</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Causal_Graphical_Model"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Probabilistic graphical models used to encode assumptions about the data-generating process.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Causal_graph</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/CenterCrop_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/CenterCrop_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Image_Preprocessing_Layer"/>
        <obo:IAO_0000115>A preprocessing layer which crops images. This layers crops the central portion of the images to a target size. If an image is smaller than the target size, it will be resized and cropped so as to return the largest possible window in the image that matches the target aspect ratio. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats.</obo:IAO_0000115>
        <rdfs:label>CenterCrop Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/CenterCrop_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which crops images. This layers crops the central portion of the images to a target size. If an image is smaller than the target size, it will be resized and cropped so as to return the largest possible window in the image that matches the target aspect ratio. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/CenterCrop</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Classification -->

    <owl:Class rdf:about="https://w3id.org/aio/Classification">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Supervised_Learning"/>
        <obo:IAO_0000115>Methods that distinguishand distribute kinds of &quot;things&quot; into different groups.</obo:IAO_0000115>
        <rdfs:label>Classification</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Classification"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that distinguishand distribute kinds of &quot;things&quot; into different groups.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Classification_(general_theory)</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Clustering -->

    <owl:Class rdf:about="https://w3id.org/aio/Clustering">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Methods that group a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Cluster analysis</oboInOwl:hasExactSynonym>
        <rdfs:label>Clustering</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Clustering"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that group a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Cluster_analysis</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Cognitive_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Cognitive_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>A broad term referring generally to a systematic pattern of deviation from rational judgement and decision-making. A large variety of cognitive biases have been identified over many decades of research in judgement and decision-making, some of which are adaptive mental shortcuts known as heuristics.</obo:IAO_0000115>
        <rdfs:label>Cognitive Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Cognitive_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A broad term referring generally to a systematic pattern of deviation from rational judgement and decision-making. A large variety of cognitive biases have been identified over many decades of research in judgement and decision-making, some of which are adaptive mental shortcuts known as heuristics.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Computational_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Computational_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Bias"/>
        <obo:IAO_0000115>A systematic tendency which causes differences between results and facts. The bias exists in numbers of the process of data analysis, including the source of the data, the estimator chosen, and the ways the data was analyzed.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Statistical Bias</oboInOwl:hasExactSynonym>
        <rdfs:label>Computational Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Computational_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A systematic tendency which causes differences between results and facts. The bias exists in numbers of the process of data analysis, including the source of the data, the estimator chosen, and the ways the data was analyzed.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Bias_(statistics)</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Concatenate_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Concatenate_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Merging_Layer"/>
        <obo:IAO_0000115>Layer that concatenates a list of inputs. It takes as input a list of tensors, all of the same shape except for the concatenation axis, and returns a single tensor that is the concatenation of all inputs.</obo:IAO_0000115>
        <rdfs:label>Concatenate Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Concatenate_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer that concatenates a list of inputs. It takes as input a list of tensors, all of the same shape except for the concatenation axis, and returns a single tensor that is the concatenation of all inputs.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Concept_Drift_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Concept_Drift_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Use_And_Interpretation_Bias"/>
        <obo:IAO_0000115>Use of a system outside the planned domain of application, and a common cause of performance gaps between laboratory settings and the real world.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Concept Drift</oboInOwl:hasExactSynonym>
        <rdfs:label>Concept Drift Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Concept_Drift_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Use of a system outside the planned domain of application, and a common cause of performance gaps between laboratory settings and the real world.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Confirmation_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Confirmation_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>A cognitive bias where people tend to prefer information that aligns with, or confirms, their existing beliefs. People can exhibit confirmation bias in the search for, interpretation of, and recall of information. In the famous Wason selection task experiments, participants repeatedly showed a preference for confirmation over falsification. They were tasked with identifying an underlying rule that applied to number triples they were shown, and they overwhelmingly tested triples that confirmed rather than falsified their hypothesized rule.</obo:IAO_0000115>
        <rdfs:label>Confirmation Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Confirmation_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A cognitive bias where people tend to prefer information that aligns with, or confirms, their existing beliefs. People can exhibit confirmation bias in the search for, interpretation of, and recall of information. In the famous Wason selection task experiments, participants repeatedly showed a preference for confirmation over falsification. They were tasked with identifying an underlying rule that applied to number triples they were shown, and they overwhelmingly tested triples that confirmed rather than falsified their hypothesized rule.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Consumer_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Consumer_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>Arises when an algorithm or platform provides users with a new venue within which to express their biases, and may occur from either side, or party, in a digital interaction..</obo:IAO_0000115>
        <rdfs:label>Consumer Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Consumer_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises when an algorithm or platform provides users with a new venue within which to express their biases, and may occur from either side, or party, in a digital interaction..</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Content_Production_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Content_Production_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Use_And_Interpretation_Bias"/>
        <obo:IAO_0000115>Arises from structural, lexical, semantic, and syntactic differences in the contents generated by users.</obo:IAO_0000115>
        <rdfs:label>Content Production Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Content_Production_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises from structural, lexical, semantic, and syntactic differences in the contents generated by users.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Continual_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Continual_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>A concept to learn a model for a large number of tasks sequentially without forgetting knowledge obtained from the preceding tasks, where the data in the old tasks are not available any more during training new ones.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Incremental Learning</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Life-Long Learning</oboInOwl:hasExactSynonym>
        <rdfs:label>Continual Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Continual_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A concept to learn a model for a large number of tasks sequentially without forgetting knowledge obtained from the preceding tasks, where the data in the old tasks are not available any more during training new ones.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://paperswithcode.com/task/continual-learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Contrastive_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Contrastive_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>Learning that encourages augmentations (views) of the same input to have more similar representations compared to augmentations of different inputs.</obo:IAO_0000115>
        <rdfs:label>Contrastive Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Contrastive_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Learning that encourages augmentations (views) of the same input to have more similar representations compared to augmentations of different inputs.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://arxiv.org/abs/2202.14037</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/ConvLSTM1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/ConvLSTM1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Convolutional_Layer"/>
        <obo:IAO_0000115>1D Convolutional LSTM. Similar to an LSTM layer, but the input transformations and recurrent transformations are both convolutional.</obo:IAO_0000115>
        <rdfs:label>ConvLSTM1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/ConvLSTM1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>1D Convolutional LSTM. Similar to an LSTM layer, but the input transformations and recurrent transformations are both convolutional.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/ConvLSTM1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/ConvLSTM2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/ConvLSTM2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Convolutional_Layer"/>
        <obo:IAO_0000115>2D Convolutional LSTM. Similar to an LSTM layer, but the input transformations and recurrent transformations are both convolutional.</obo:IAO_0000115>
        <rdfs:label>ConvLSTM2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/ConvLSTM2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>2D Convolutional LSTM. Similar to an LSTM layer, but the input transformations and recurrent transformations are both convolutional.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/ConvLSTM2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/ConvLSTM3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/ConvLSTM3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Convolutional_Layer"/>
        <obo:IAO_0000115>3D Convolutional LSTM. Similar to an LSTM layer, but the input transformations and recurrent transformations are both convolutional.</obo:IAO_0000115>
        <rdfs:label>ConvLSTM3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/ConvLSTM3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>3D Convolutional LSTM. Similar to an LSTM layer, but the input transformations and recurrent transformations are both convolutional.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/ConvLSTM3D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Convolution1DTranspose_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Convolution1DTranspose_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Transposed convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers or None, does not include the sample axis), e.g. input_shape=(128, 3) for data with 128 time steps and 3 channels.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Conv1DTranspose Layer</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>ConvTranspose1d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolution1DTranspose</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolution1dTranspose</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.ConvTranspose1d</oboInOwl:hasExactSynonym>
        <rdfs:label>Convolution1DTranspose Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Convolution1DTranspose_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Transposed convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers or None, does not include the sample axis), e.g. input_shape=(128, 3) for data with 128 time steps and 3 channels.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1DTranspose</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Convolution1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Convolution1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>1D convolution layer (e.g. temporal convolution).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Conv1D Layer</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Conv1d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolution1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolution1d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.Conv1d</oboInOwl:hasExactSynonym>
        <rdfs:label>Convolution1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Convolution1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>1D convolution layer (e.g. temporal convolution).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Convolution2DTranspose_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Convolution2DTranspose_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Transposed convolution layer (sometimes called Deconvolution).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Conv2DTranspose Layer</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>ConvTranspose2d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolution2DTranspose</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolution2dTranspose</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.ConvTranspose2d</oboInOwl:hasExactSynonym>
        <rdfs:label>Convolution2DTranspose Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Convolution2DTranspose_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Transposed convolution layer (sometimes called Deconvolution).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Convolution2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Convolution2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>2D convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers or None, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=&quot;channels_last&quot;. You can use None when a dimension has variable size.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Conv2D Layer</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Conv2d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolution2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolution2d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.Conv2d</oboInOwl:hasExactSynonym>
        <rdfs:label>Convolution2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Convolution2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>2D convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers or None, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=&quot;channels_last&quot;. You can use None when a dimension has variable size.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Convolution3DTranspose_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Convolution3DTranspose_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Transposed convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers or None, does not include the sample axis), e.g. input_shape=(128, 128, 128, 3) for a 128x128x128 volume with 3 channels if data_format=&quot;channels_last&quot;.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Conv3DTranspose Layer</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>ConvTranspose3d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolution3DTranspose</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolution3dTranspose</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.ConvTranspose3d</oboInOwl:hasExactSynonym>
        <rdfs:label>Convolution3DTranspose Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Convolution3DTranspose_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Transposed convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers or None, does not include the sample axis), e.g. input_shape=(128, 128, 128, 3) for a 128x128x128 volume with 3 channels if data_format=&quot;channels_last&quot;.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv3DTranspose</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Convolution3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Convolution3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>3D convolution layer (e.g. spatial convolution over volumes). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers or None, does not include the sample axis), e.g. input_shape=(128, 128, 128, 1) for 128x128x128 volumes with a single channel, in data_format=&quot;channels_last&quot;.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Conv3D Layer</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Conv3d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolution3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolution3d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.Conv3d</oboInOwl:hasExactSynonym>
        <rdfs:label>Convolution3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Convolution3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>3D convolution layer (e.g. spatial convolution over volumes). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers or None, does not include the sample axis), e.g. input_shape=(128, 128, 128, 1) for 128x128x128 volumes with a single channel, in data_format=&quot;channels_last&quot;.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv3D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Convolutional_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Convolutional_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A convolutional layer is the main building block of a CNN. It contains a set of filters (or kernels), parameters of which are to be learned throughout the training. The size of the filters is usually smaller than the actual image. Each filter convolves with the image and creates an activation map.</obo:IAO_0000115>
        <rdfs:label>Convolutional Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Convolutional_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A convolutional layer is the main building block of a CNN. It contains a set of filters (or kernels), parameters of which are to be learned throughout the training. The size of the filters is usually smaller than the actual image. Each filter convolves with the image and creates an activation map.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.sciencedirect.com/topics/engineering/convolutional-layer#:~:text=A%20convolutional%20layer%20is%20the,and%20creates%20an%20activation%20map.</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Cropping1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Cropping1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Reshaping_Layer"/>
        <obo:IAO_0000115>Cropping layer for 1D input (e.g. temporal sequence). It crops along the time dimension (axis 1).</obo:IAO_0000115>
        <rdfs:label>Cropping1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Cropping1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Cropping layer for 1D input (e.g. temporal sequence). It crops along the time dimension (axis 1).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Cropping1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Cropping2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Cropping2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Cropping layer for 2D input (e.g. picture). It crops along spatial dimensions, i.e. height and width.</obo:IAO_0000115>
        <rdfs:label>Cropping2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Cropping2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Cropping layer for 2D input (e.g. picture). It crops along spatial dimensions, i.e. height and width.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Cropping2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Cropping3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Cropping3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Cropping layer for 3D data (e.g. spatial or spatio-temporal).</obo:IAO_0000115>
        <rdfs:label>Cropping3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Cropping3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Cropping layer for 3D data (e.g. spatial or spatio-temporal).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Cropping3D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/DFF -->

    <owl:Class rdf:about="https://w3id.org/aio/DFF"/>
    


    <!-- https://w3id.org/aio/DNN -->

    <owl:Class rdf:about="https://w3id.org/aio/DNN"/>
    


    <!-- https://w3id.org/aio/Data_Dredging_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Data_Dredging_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Use_And_Interpretation_Bias"/>
        <obo:IAO_0000115>A statistical bias in which testing huge numbers of hypotheses of a dataset may appear to yield statistical significance even when the results are statistically nonsignificant.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Data Dredging</oboInOwl:hasExactSynonym>
        <rdfs:label>Data Dredging Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Data_Dredging_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A statistical bias in which testing huge numbers of hypotheses of a dataset may appear to yield statistical significance even when the results are statistically nonsignificant.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Data_Generation_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Data_Generation_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <obo:IAO_0000115>Arises from the addition of synthetic or redundant data samples to a dataset.</obo:IAO_0000115>
        <rdfs:label>Data Generation Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Data_Generation_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises from the addition of synthetic or redundant data samples to a dataset.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Data_Imputation -->

    <owl:Class rdf:about="https://w3id.org/aio/Data_Imputation">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Methods that replace missing data with substituted values.</obo:IAO_0000115>
        <rdfs:label>Data Imputation</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Data_Imputation"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that replace missing data with substituted values.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Imputation_(statistics)</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Decision_Tree -->

    <owl:Class rdf:about="https://w3id.org/aio/Decision_Tree">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Classification"/>
        <obo:IAO_0000115>A decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility.</obo:IAO_0000115>
        <rdfs:label>Decision Tree</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Decision_Tree"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Decision_tree</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Decoder_LLM -->

    <owl:Class rdf:about="https://w3id.org/aio/Decoder_LLM">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/LLM"/>
        <obo:IAO_0000115>In the decoder-only architecture, the model consists of only a decoder, which is trained to predict the next token in a sequence given the previous tokens. The critical difference between the Decoder-only architecture and the Encoder-Decoder architecture is that the Decoder-only architecture does not have an explicit encoder to summarize the input information. Instead, the information is encoded implicitly in the hidden state of the decoder, which is updated at each step of the generation process.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LLM</oboInOwl:hasExactSynonym>
        <rdfs:label>Decoder LLM</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Decoder_LLM"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>In the decoder-only architecture, the model consists of only a decoder, which is trained to predict the next token in a sequence given the previous tokens. The critical difference between the Decoder-only architecture and the Encoder-Decoder architecture is that the Decoder-only architecture does not have an explicit encoder to summarize the input information. Instead, the information is encoded implicitly in the hidden state of the decoder, which is updated at each step of the generation process.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.practicalai.io/understanding-transformer-model-architectures/#:~:text=Encoder%2Donly&amp;text=These%20models%20have%20a%20pre,Named%20entity%20recognition</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Deconvolutional_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Deconvolutional_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>Deconvolutional Networks, a framework that permits the unsupervised construction of hierarchical image representations. These representations can be used for both low-level tasks such as denoising, as well as providing features for object recognition. Each level of the hierarchy groups information from the level beneath to form more complex features that exist over a larger scale in the image. (https://ieeexplore.ieee.org/document/5539957)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>DN</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Kernel, Convolutional/Pool, Output</rdfs:comment>
        <rdfs:label>Deconvolutional Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Deconvolutional_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Deconvolutional Networks, a framework that permits the unsupervised construction of hierarchical image representations. These representations can be used for both low-level tasks such as denoising, as well as providing features for object recognition. Each level of the hierarchy groups information from the level beneath to form more complex features that exist over a larger scale in the image. (https://ieeexplore.ieee.org/document/5539957)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://ieeexplore.ieee.org/document/5539957</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Deep_Active_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Deep_Active_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>The combination of deep learning and active learning, where active learning attempts to maximize a model’s performance gain while annotating the fewest samples possible.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>DeepAL</oboInOwl:hasExactSynonym>
        <rdfs:label>Deep Active Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Deep_Active_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The combination of deep learning and active learning, where active learning attempts to maximize a model’s performance gain while annotating the fewest samples possible.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://arxiv.org/pdf/2009.00236.pdf</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Deep_Belief_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Deep_Belief_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/UPN"/>
        <obo:IAO_0000115>In machine Learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (&quot;hidden units&quot;), with connections between the layers but not between units within each layer. When trained on a set of examples without supervision, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors. After this Learning step, a DBN can be further trained with supervision to perform classification. DBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs) or autoencoders, where each sub-network&apos;s hidden layer serves as the visible layer for the next. An RBM is an undirected, generative energy-based model with a &quot;visible&quot; input layer and a hidden layer and connections between but not within layers. This composition leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the &quot;lowest&quot; pair of layers (the lowest visible layer is a training set). The observation that DBNs can be trained greedily, one layer at a time, led to one of the first effective deep Learning algorithms. (https://en.wikipedia.org/wiki/Deep_belief_network)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>DBN</oboInOwl:hasExactSynonym>
        <rdfs:comment>Backfed Input, Probabilistic Hidden, Hidden, Matched Output-Input</rdfs:comment>
        <rdfs:label>Deep Belief Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Deep_Belief_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>In machine Learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (&quot;hidden units&quot;), with connections between the layers but not between units within each layer. When trained on a set of examples without supervision, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors. After this Learning step, a DBN can be further trained with supervision to perform classification. DBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs) or autoencoders, where each sub-network&apos;s hidden layer serves as the visible layer for the next. An RBM is an undirected, generative energy-based model with a &quot;visible&quot; input layer and a hidden layer and connections between but not within layers. This composition leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the &quot;lowest&quot; pair of layers (the lowest visible layer is a training set). The observation that DBNs can be trained greedily, one layer at a time, led to one of the first effective deep Learning algorithms. (https://en.wikipedia.org/wiki/Deep_belief_network)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Deep_belief_network</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Deep_Convolutional_Inverse_Graphics_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Deep_Convolutional_Inverse_Graphics_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/AE"/>
        <obo:IAO_0000115>A Deep Convolution Inverse Graphics Network (DC-IGN) is a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. (https://arxiv.org/abs/1503.03167)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>DCIGN</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Kernel, Convolutional/Pool, Probabilistic Hidden, Convolutional/Pool, Kernel, Output</rdfs:comment>
        <rdfs:label>Deep Convolutional Inverse Graphics Network</rdfs:label>
    </owl:Class>
    


    <!-- https://w3id.org/aio/Deep_Convolutional_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Deep_Convolutional_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>A convolutional neural network (CNN, or ConvNet) is a class of artificial neural network, most commonly applied to analyze visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation equivariant responses known as feature maps. CNNs are regularized versions of multilayer perceptrons. (https://en.wikipedia.org/wiki/Convolutional_neural_network)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>CNN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>ConvNet</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Convolutional Neural Network</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>DCN</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Kernel, Convolutional/Pool, Hidden, Output</rdfs:comment>
        <rdfs:label>Deep Convolutional Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Deep_Convolutional_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A convolutional neural network (CNN, or ConvNet) is a class of artificial neural network, most commonly applied to analyze visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation equivariant responses known as feature maps. CNNs are regularized versions of multilayer perceptrons. (https://en.wikipedia.org/wiki/Convolutional_neural_network)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Convolutional_neural_network</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Deep_FeedFoward -->

    <owl:Class rdf:about="https://w3id.org/aio/Deep_FeedFoward">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>The feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>DFF</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>FFN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Feedforward Network</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MLP</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Multilayer Perceptoron</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Hidden, Output</rdfs:comment>
        <rdfs:label>Deep FeedFoward</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Deep_FeedFoward"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Feedforward_neural_network</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Deep_Neural_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Deep_Neural_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/ANN"/>
        <obo:IAO_0000115>A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers.[13][2] There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. (https://en.wikipedia.org/wiki/Deep_Learning#:~:text=A%20deep%20neural%20network%20(DNN,weights%2C%20biases%2C%20and%20functions.)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>DNN</oboInOwl:hasExactSynonym>
        <rdfs:label>Deep Neural Network</rdfs:label>
    </owl:Class>
    


    <!-- https://w3id.org/aio/Deep_Transfer_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Deep_Transfer_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>Deep transfer learning methods relax the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data.</obo:IAO_0000115>
        <rdfs:label>Deep Transfer Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Deep_Transfer_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Deep transfer learning methods relax the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://arxiv.org/abs/1808.01974</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Denoising_Auto_Encoder -->

    <owl:Class rdf:about="https://w3id.org/aio/Denoising_Auto_Encoder">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/AE"/>
        <obo:IAO_0000115>Denoising Auto Encoders (DAEs) take a partially corrupted input and are trained to recover the original undistorted input. In practice, the objective of denoising autoencoders is that of cleaning the corrupted input, or denoising. (https://en.wikipedia.org/wiki/Autoencoder)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>DAE</oboInOwl:hasExactSynonym>
        <rdfs:comment>Noisy Input, Hidden, Matched Output-Input</rdfs:comment>
        <rdfs:label>Denoising Auto Encoder</rdfs:label>
    </owl:Class>
    


    <!-- https://w3id.org/aio/DenseFeatures_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/DenseFeatures_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A layer that produces a dense Tensor based on given feature_columns. Generally a single example in training data is described with FeatureColumns. At the first layer of the model, this column oriented data should be converted to a single Tensor. This layer can be called multiple times with different features. This is the V2 version of this layer that uses name_scopes to create variables instead of variable_scopes. But this approach currently lacks support for partitioned variables. In that case, use the V1 version instead.</obo:IAO_0000115>
        <rdfs:label>DenseFeatures Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/DenseFeatures_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A layer that produces a dense Tensor based on given feature_columns. Generally a single example in training data is described with FeatureColumns. At the first layer of the model, this column oriented data should be converted to a single Tensor. This layer can be called multiple times with different features. This is the V2 version of this layer that uses name_scopes to create variables instead of variable_scopes. But this approach currently lacks support for partitioned variables. In that case, use the V1 version instead.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/DenseFeatures</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Dense_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Dense_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Just your regular densely-connected NN layer.</obo:IAO_0000115>
        <rdfs:label>Dense Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Dense_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Just your regular densely-connected NN layer.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Deployment_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Deployment_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Group_Bias"/>
        <obo:IAO_0000115>Arises when systems are used as decision aids for humans, since the human intermediary may act on predictions in ways that are typically not modeled in the system. However, it is still individuals using the deployed system.</obo:IAO_0000115>
        <rdfs:label>Deployment Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Deployment_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises when systems are used as decision aids for humans, since the human intermediary may act on predictions in ways that are typically not modeled in the system. However, it is still individuals using the deployed system.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/DepthwiseConv1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/DepthwiseConv1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Convolutional_Layer"/>
        <obo:IAO_0000115>Depthwise 1D convolution. Depthwise convolution is a type of convolution in which each input channel is convolved with a different kernel (called a depthwise kernel). You can understand depthwise convolution as the first step in a depthwise separable convolution. It is implemented via the following steps: Split the input into individual channels. Convolve each channel with an individual depthwise kernel with depth_multiplier output channels. Concatenate the convolved outputs along the channels axis. Unlike a regular 1D convolution, depthwise convolution does not mix information across different input channels. The depth_multiplier argument determines how many filter are applied to one input channel. As such, it controls the amount of output channels that are generated per input channel in the depthwise step.</obo:IAO_0000115>
        <rdfs:label>DepthwiseConv1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/DepthwiseConv1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Depthwise 1D convolution. Depthwise convolution is a type of convolution in which each input channel is convolved with a different kernel (called a depthwise kernel). You can understand depthwise convolution as the first step in a depthwise separable convolution. It is implemented via the following steps: Split the input into individual channels. Convolve each channel with an individual depthwise kernel with depth_multiplier output channels. Concatenate the convolved outputs along the channels axis. Unlike a regular 1D convolution, depthwise convolution does not mix information across different input channels. The depth_multiplier argument determines how many filter are applied to one input channel. As such, it controls the amount of output channels that are generated per input channel in the depthwise step.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/DepthwiseConv2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/DepthwiseConv2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Convolutional_Layer"/>
        <obo:IAO_0000115>Depthwise 2D convolution.</obo:IAO_0000115>
        <rdfs:label>DepthwiseConv2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/DepthwiseConv2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Depthwise 2D convolution.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Detection_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Detection_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <obo:IAO_0000115>Systematic differences between groups in how outcomes are determined and may cause an over- or underestimation of the size of the effect.</obo:IAO_0000115>
        <rdfs:label>Detection Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Detection_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Systematic differences between groups in how outcomes are determined and may cause an over- or underestimation of the size of the effect.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Dimensionality_Reduction -->

    <owl:Class rdf:about="https://w3id.org/aio/Dimensionality_Reduction">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Unsupervised_Learning"/>
        <obo:IAO_0000115>The transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Dimension Reduction</oboInOwl:hasExactSynonym>
        <rdfs:label>Dimensionality Reduction</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Dimensionality_Reduction"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Dimensionality_reduction</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Discretization_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Discretization_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Numerical_Features_Preprocessing_Layer"/>
        <obo:IAO_0000115>A preprocessing layer which buckets continuous features by ranges.</obo:IAO_0000115>
        <rdfs:label>Discretization Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Discretization_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which buckets continuous features by ranges.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Discretization</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Dot_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Dot_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Layer that computes a dot product between samples in two tensors. E.g. if applied to a list of two tensors a and b of shape (batch_size, n), the output will be a tensor of shape (batch_size, 1) where each entry i will be the dot product between a[i] and b[i].</obo:IAO_0000115>
        <rdfs:label>Dot Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Dot_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer that computes a dot product between samples in two tensors. E.g. if applied to a list of two tensors a and b of shape (batch_size, n), the output will be a tensor of shape (batch_size, 1) where each entry i will be the dot product between a[i] and b[i].</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dot</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Dropout_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Dropout_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regularization_Layer"/>
        <obo:IAO_0000115>Applies Dropout to the input. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged. Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit, training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer. (This is in contrast to setting trainable=False for a Dropout layer. trainable does not affect the layer&apos;s behavior, as Dropout does not have any variables/weights that can be frozen during training.)</obo:IAO_0000115>
        <rdfs:label>Dropout Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Dropout_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies Dropout to the input. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged. Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit, training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer. (This is in contrast to setting trainable=False for a Dropout layer. trainable does not affect the layer&apos;s behavior, as Dropout does not have any variables/weights that can be frozen during training.)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Dunning-Kruger_Effect_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Dunning-Kruger_Effect_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Cognitive_Bias"/>
        <obo:IAO_0000115>The tendency of people with low ability in a given area or task to overestimate their self-assessed ability. Typically measured by comparing self-assessment with objective performance, often called subjective ability and objective ability, respectively.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Dunning-Kruger Effect</oboInOwl:hasExactSynonym>
        <rdfs:label>Dunning-Kruger Effect Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Dunning-Kruger_Effect_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The tendency of people with low ability in a given area or task to overestimate their self-assessed ability. Typically measured by comparing self-assessment with objective performance, often called subjective ability and objective ability, respectively.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/ELU_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/ELU_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Activation_Layer"/>
        <obo:IAO_0000115>Exponential Linear Unit.</obo:IAO_0000115>
        <rdfs:label>ELU Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/ELU_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Exponential Linear Unit.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/ELU</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Echo_State_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Echo_State_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/RecNN"/>
        <obo:IAO_0000115>The echo state network (ESN) is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>ESN</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Recurrent, Output</rdfs:comment>
        <rdfs:label>Echo State Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Echo_State_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The echo state network (ESN) is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Echo_state_network#:~:text=The%20echo%20state%20network%20(ESN,are%20fixed%20and%20randomly%20assigned</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Ecological_Fallacy_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Ecological_Fallacy_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <obo:IAO_0000115>Occurs when an inference is made about an individual based on their membership within a group.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Ecological Fallacy</oboInOwl:hasExactSynonym>
        <rdfs:label>Ecological Fallacy Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Ecological_Fallacy_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Occurs when an inference is made about an individual based on their membership within a group.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Embedding_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Embedding_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Turns positive integers (indexes) into dense vectors of fixed size.</obo:IAO_0000115>
        <rdfs:label>Embedding Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Embedding_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Turns positive integers (indexes) into dense vectors of fixed size.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Emergent_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Emergent_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Use_And_Interpretation_Bias"/>
        <obo:IAO_0000115>Emergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts.</obo:IAO_0000115>
        <rdfs:label>Emergent Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Emergent_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Emergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Encoder-Decoder_LLM -->

    <owl:Class rdf:about="https://w3id.org/aio/Encoder-Decoder_LLM">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/LLM"/>
        <obo:IAO_0000115>The Encoder-Decoder architecture was the original transformer architecture introduced in the Attention Is All You Need (https://arxiv.org/abs/1706.03762) paper. The encoder processes the input sequence and generates a hidden representation that summarizes the input information. The decoder uses this hidden representation to generate the desired output sequence. The encoder and decoder are trained end-to-end to maximize the likelihood of the correct output sequence given the input sequence.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LLM</oboInOwl:hasExactSynonym>
        <rdfs:label>Encoder-Decoder LLM</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Encoder-Decoder_LLM"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The Encoder-Decoder architecture was the original transformer architecture introduced in the Attention Is All You Need (https://arxiv.org/abs/1706.03762) paper. The encoder processes the input sequence and generates a hidden representation that summarizes the input information. The decoder uses this hidden representation to generate the desired output sequence. The encoder and decoder are trained end-to-end to maximize the likelihood of the correct output sequence given the input sequence.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.practicalai.io/understanding-transformer-model-architectures/#:~:text=Encoder%2Donly&amp;text=These%20models%20have%20a%20pre,Named%20entity%20recognition</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Encoder_LLM -->

    <owl:Class rdf:about="https://w3id.org/aio/Encoder_LLM">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/LLM"/>
        <obo:IAO_0000115>The Encoder-only architecture is used when only encoding the input sequence is required and the decoder is not necessary. The input sequence is encoded into a fixed-length representation and then used as input to a classifier or a regressor to make a prediction. These models have a pre-trained general-purpose encoder but will require fine-tuning of the final classifier or regressor.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LLM</oboInOwl:hasExactSynonym>
        <rdfs:label>Encoder LLM</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Encoder_LLM"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The Encoder-only architecture is used when only encoding the input sequence is required and the decoder is not necessary. The input sequence is encoded into a fixed-length representation and then used as input to a classifier or a regressor to make a prediction. These models have a pre-trained general-purpose encoder but will require fine-tuning of the final classifier or regressor.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.practicalai.io/understanding-transformer-model-architectures/#:~:text=Encoder%2Donly&amp;text=These%20models%20have%20a%20pre,Named%20entity%20recognition</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Ensemble_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Ensemble_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.</obo:IAO_0000115>
        <rdfs:label>Ensemble Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Ensemble_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Ensemble_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Error_Propagation_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Error_Propagation_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Processing_Bias"/>
        <obo:IAO_0000115>The effect of variables&apos; uncertainties (or errors, more specifically random errors) on the uncertainty of a function based on them.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Error Propagation</oboInOwl:hasExactSynonym>
        <rdfs:label>Error Propagation Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Error_Propagation_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The effect of variables&apos; uncertainties (or errors, more specifically random errors) on the uncertainty of a function based on them.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Evaluation_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Evaluation_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <obo:IAO_0000115>Arises when the testing or external benchmark populations do not equally represent the various parts of the user population or from the use of performance metrics that are not appropriate for the way in which the model will be used.</obo:IAO_0000115>
        <rdfs:label>Evaluation Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Evaluation_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises when the testing or external benchmark populations do not equally represent the various parts of the user population or from the use of performance metrics that are not appropriate for the way in which the model will be used.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Exclusion_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Exclusion_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <obo:IAO_0000115>When specific groups of user populations are excluded from testing and subsequent analyses.</obo:IAO_0000115>
        <rdfs:label>Exclusion Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Exclusion_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>When specific groups of user populations are excluded from testing and subsequent analyses.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Exponential_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/Exponential_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>The exponential function is a mathematical function denoted by f(x)=exp or e^{x}.</obo:IAO_0000115>
        <rdfs:label>Exponential Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Exponential_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The exponential function is a mathematical function denoted by f(x)=exp or e^{x}.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/exponential</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Extreme_Learning_Machine -->

    <owl:Class rdf:about="https://w3id.org/aio/Extreme_Learning_Machine">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/FBN"/>
        <obo:IAO_0000115>Extreme Learning machines are feedforward neural networks for classification, regression, clustering, sparse approximation, compression and feature Learning with a single layer or multiple layers of hidden nodes, where the parameters of hidden nodes (not just the weights connecting inputs to hidden nodes) need not be tuned. These hidden nodes can be randomly assigned and never updated (i.e. they are random projection but with nonlinear transforms), or can be inherited from their ancestors without being changed. In most cases, the output weights of hidden nodes are usually learned in a single step, which essentially amounts to Learning a linear model. (https://en.wikipedia.org/wiki/Extreme_Learning_machine)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>ELM</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Hidden, Output</rdfs:comment>
        <rdfs:label>Extreme Learning Machine</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Extreme_Learning_Machine"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Extreme Learning machines are feedforward neural networks for classification, regression, clustering, sparse approximation, compression and feature Learning with a single layer or multiple layers of hidden nodes, where the parameters of hidden nodes (not just the weights connecting inputs to hidden nodes) need not be tuned. These hidden nodes can be randomly assigned and never updated (i.e. they are random projection but with nonlinear transforms), or can be inherited from their ancestors without being changed. In most cases, the output weights of hidden nodes are usually learned in a single step, which essentially amounts to Learning a linear model. (https://en.wikipedia.org/wiki/Extreme_Learning_machine)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Extreme_Learning_machine</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/FBN -->

    <owl:Class rdf:about="https://w3id.org/aio/FBN"/>
    


    <!-- https://w3id.org/aio/Federated_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Federated_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>A technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them.</obo:IAO_0000115>
        <rdfs:label>Federated Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Federated_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Federated_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Feedback_Loop_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Feedback_Loop_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Use_And_Interpretation_Bias"/>
        <obo:IAO_0000115>Effects that may occur when an algorithm learns from user behavior and feeds that behavior back into the model.</obo:IAO_0000115>
        <rdfs:label>Feedback Loop Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Feedback_Loop_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Effects that may occur when an algorithm learns from user behavior and feeds that behavior back into the model.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Feedback_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Feedback_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/ANN"/>
        <obo:IAO_0000115>A feedback based approach in which the representation is formed in an iterative manner based on a feedback received from previous iteration&apos;s output. (https://arxiv.org/abs/1612.09508)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>FBN</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Hidden, Output, Hidden</rdfs:comment>
        <rdfs:label>Feedback Network</rdfs:label>
    </owl:Class>
    


    <!-- https://w3id.org/aio/Fixed_Effects_Model -->

    <owl:Class rdf:about="https://w3id.org/aio/Fixed_Effects_Model">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regression_Analysis"/>
        <obo:IAO_0000115>A statistical model in which the model parameters are fixed or non-random quantities.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>FEM</oboInOwl:hasExactSynonym>
        <rdfs:label>Fixed Effects Model</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Fixed_Effects_Model"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A statistical model in which the model parameters are fixed or non-random quantities.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Fixed_effects_model</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Flatten_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Flatten_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Reshaping_Layer"/>
        <obo:IAO_0000115>Flattens the input. Does not affect the batch size.</obo:IAO_0000115>
        <rdfs:label>Flatten Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Flatten_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Flattens the input. Does not affect the batch size.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/FractionalMaxPool2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/FractionalMaxPool2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 2D fractional max pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>FractionalMaxPool2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>FractionalMaxPool2d</oboInOwl:hasExactSynonym>
        <rdfs:label>FractionalMaxPool2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/FractionalMaxPool2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 2D fractional max pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/FractionalMaxPool3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/FractionalMaxPool3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 3D fractional max pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>FractionalMaxPool3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>FractionalMaxPool3d</oboInOwl:hasExactSynonym>
        <rdfs:label>FractionalMaxPool3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/FractionalMaxPool3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 3D fractional max pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Function -->

    <owl:Class rdf:about="https://w3id.org/aio/Function"/>
    


    <!-- https://w3id.org/aio/Funding_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Funding_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Group_Bias"/>
        <obo:IAO_0000115>Arises when biased results are reported in order to support or satisfy the funding agency or financial supporter of the research study, but it can also be the individual researcher.</obo:IAO_0000115>
        <rdfs:label>Funding Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Funding_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises when biased results are reported in order to support or satisfy the funding agency or financial supporter of the research study, but it can also be the individual researcher.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/GCN -->

    <owl:Class rdf:about="https://w3id.org/aio/GCN"/>
    


    <!-- https://w3id.org/aio/GRUCell_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/GRUCell_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Cell class for the GRU layer. This class processes one step within the whole time sequence input, whereas tf.keras.layer.GRU processes the whole sequence.</obo:IAO_0000115>
        <rdfs:label>GRUCell Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/GRUCell_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Cell class for the GRU layer. This class processes one step within the whole time sequence input, whereas tf.keras.layer.GRU processes the whole sequence.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/GRU_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/GRU_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Recurrent_Layer"/>
        <obo:IAO_0000115>Gated Recurrent Unit - Cho et al. 2014. Based on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or pure-TensorFlow) to maximize the performance. If a GPU is available and all the arguments to the layer meet the requirement of the cuDNN kernel (see below for details), the layer will use a fast cuDNN implementation. The requirements to use the cuDNN implementation are: activation == tanh, recurrent_activation == sigmoid, recurrent_dropout == 0, unroll is False, use_bias is True, reset_after is True. Inputs, if use masking, are strictly right-padded. Eager execution is enabled in the outermost context. There are two variants of the GRU implementation. The default one is based on v3 and has reset gate applied to hidden state before matrix multiplication. The other one is based on original and has the order reversed. The second variant is compatible with CuDNNGRU (GPU-only) and allows inference on CPU. Thus it has separate biases for kernel and recurrent_kernel. To use this variant, set reset_after=True and recurrent_activation=&apos;sigmoid&apos;.</obo:IAO_0000115>
        <rdfs:label>GRU Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/GRU_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Gated Recurrent Unit - Cho et al. 2014. Based on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or pure-TensorFlow) to maximize the performance. If a GPU is available and all the arguments to the layer meet the requirement of the cuDNN kernel (see below for details), the layer will use a fast cuDNN implementation. The requirements to use the cuDNN implementation are: activation == tanh, recurrent_activation == sigmoid, recurrent_dropout == 0, unroll is False, use_bias is True, reset_after is True. Inputs, if use masking, are strictly right-padded. Eager execution is enabled in the outermost context. There are two variants of the GRU implementation. The default one is based on v3 and has reset gate applied to hidden state before matrix multiplication. The other one is based on original and has the order reversed. The second variant is compatible with CuDNNGRU (GPU-only) and allows inference on CPU. Thus it has separate biases for kernel and recurrent_kernel. To use this variant, set reset_after=True and recurrent_activation=&apos;sigmoid&apos;.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Gated_Recurrent_Unit -->

    <owl:Class rdf:about="https://w3id.org/aio/Gated_Recurrent_Unit">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/LSTM"/>
        <obo:IAO_0000115>Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. GRU&apos;s performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM.[4][5] GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GRU</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Memory Cell, Output</rdfs:comment>
        <rdfs:label>Gated Recurrent Unit</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Gated_Recurrent_Unit"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. GRU&apos;s performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM.[4][5] GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Gated_recurrent_unit</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/GaussianDropout_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/GaussianDropout_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regularization_Layer"/>
        <obo:IAO_0000115>Apply multiplicative 1-centered Gaussian noise. As it is a regularization layer, it is only active at training time.</obo:IAO_0000115>
        <rdfs:label>GaussianDropout Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/GaussianDropout_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Apply multiplicative 1-centered Gaussian noise. As it is a regularization layer, it is only active at training time.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/GaussianDropout</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/GaussianNoise_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/GaussianNoise_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regularization_Layer"/>
        <obo:IAO_0000115>Apply additive zero-centered Gaussian noise. This is useful to mitigate overfitting (you could see it as a form of random data augmentation). Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs. As it is a regularization layer, it is only active at training time.</obo:IAO_0000115>
        <rdfs:label>GaussianNoise Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/GaussianNoise_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Apply additive zero-centered Gaussian noise. This is useful to mitigate overfitting (you could see it as a form of random data augmentation). Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs. As it is a regularization layer, it is only active at training time.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/GaussianNoise</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/GeLu_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/GeLu_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>Gaussian error linear unit (GELU) computes x * P(X &lt;= x), where P(X) ~ N(0, 1). The (GELU) nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLU.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GELU</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Gaussian Error Linear Unit</oboInOwl:hasExactSynonym>
        <rdfs:label>GELU Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/GeLu_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Gaussian error linear unit (GELU) computes x * P(X &lt;= x), where P(X) ~ N(0, 1). The (GELU) nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLU.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/gelu</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Generalized_Few-shot_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Generalized_Few-shot_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>Methods that can learn novel classes from only few samples per class, preventing catastrophic forgetting of base classes, and classifier calibration across novel and base classes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GFSL</oboInOwl:hasExactSynonym>
        <rdfs:label>Generalized Few-shot Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Generalized_Few-shot_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that can learn novel classes from only few samples per class, preventing catastrophic forgetting of base classes, and classifier calibration across novel and base classes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://paperswithcode.com/paper/generalized-and-incremental-few-shot-learning/review/</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Generalized_Linear_Model -->

    <owl:Class rdf:about="https://w3id.org/aio/Generalized_Linear_Model">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regression_Analysis"/>
        <obo:IAO_0000115>This model generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GLM</oboInOwl:hasExactSynonym>
        <rdfs:label>Generalized Linear Model</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Generalized_Linear_Model"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>This model generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Generalized_linear_model</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Generative_Adversarial_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Generative_Adversarial_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/UPN"/>
        <obo:IAO_0000115>A generative adversarial network (GAN) is a class of machine Learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent&apos;s gain is another agent&apos;s loss). Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised Learning, GANs have also proven useful for semi-supervised Learning, fully supervised Learning,[ and reinforcement Learning. The core idea of a GAN is based on the &quot;indirect&quot; training through the discriminator,[clarification needed] which itself is also being updated dynamically. This basically means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GAN</oboInOwl:hasExactSynonym>
        <rdfs:comment>Backfed Input, Hidden, Matched Output-Input, Hidden, Matched Output-Input</rdfs:comment>
        <rdfs:label>Generative Adversarial Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Generative_Adversarial_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A generative adversarial network (GAN) is a class of machine Learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent&apos;s gain is another agent&apos;s loss). Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised Learning, GANs have also proven useful for semi-supervised Learning, fully supervised Learning,[ and reinforcement Learning. The core idea of a GAN is based on the &quot;indirect&quot; training through the discriminator,[clarification needed] which itself is also being updated dynamically. This basically means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Generative_adversarial_network</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/GlobalAveragePooling1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/GlobalAveragePooling1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Global average pooling operation for temporal data.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GlobalAvgPool1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>GlobalAvgPool1d</oboInOwl:hasExactSynonym>
        <rdfs:label>GlobalAveragePooling1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/GlobalAveragePooling1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Global average pooling operation for temporal data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/GlobalAveragePooling2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/GlobalAveragePooling2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Global average pooling operation for spatial data.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GlobalAvgPool2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>GlobalAvgPool2d</oboInOwl:hasExactSynonym>
        <rdfs:label>GlobalAveragePooling2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/GlobalAveragePooling2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Global average pooling operation for spatial data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/GlobalAveragePooling3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/GlobalAveragePooling3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Global Average pooling operation for 3D data.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GlobalAvgPool3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>GlobalAvgPool3d</oboInOwl:hasExactSynonym>
        <rdfs:label>GlobalAveragePooling3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/GlobalAveragePooling3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Global Average pooling operation for 3D data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling3D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/GlobalMaxPooling1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/GlobalMaxPooling1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Global max pooling operation for 1D temporal data.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GlobalMaxPool1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>GlobalMaxPool1d</oboInOwl:hasExactSynonym>
        <rdfs:label>GlobalMaxPooling1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/GlobalMaxPooling1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Global max pooling operation for 1D temporal data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/GlobalMaxPooling2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/GlobalMaxPooling2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Global max pooling operation for spatial data.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GlobalMaxPool2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>GlobalMaxPool2d</oboInOwl:hasExactSynonym>
        <rdfs:label>GlobalMaxPooling2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/GlobalMaxPooling2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Global max pooling operation for spatial data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/GlobalMaxPooling3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/GlobalMaxPooling3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Global Max pooling operation for 3D data.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GlobalMaxPool3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>GlobalMaxPool3d</oboInOwl:hasExactSynonym>
        <rdfs:label>GlobalMaxPooling3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/GlobalMaxPooling3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Global Max pooling operation for 3D data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool3D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Graph_Convolutional_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Graph_Convolutional_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>GCN is a type of convolutional neural network that can work directly on graphs and take advantage of their structural information. (https://arxiv.org/abs/1609.02907)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GCN</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Hidden, Hidden, Output</rdfs:comment>
        <rdfs:label>Graph Convolutional Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Graph_Convolutional_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>GCN is a type of convolutional neural network that can work directly on graphs and take advantage of their structural information. (https://arxiv.org/abs/1609.02907)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://arxiv.org/abs/1609.02907</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Graph_Convolutional_Policy_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Graph_Convolutional_Policy_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/GCN"/>
        <obo:IAO_0000115>Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goal-directed graph generation through reinforcement Learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GPCN</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Hidden, Hidden, Policy, Output</rdfs:comment>
        <rdfs:label>Graph Convolutional Policy Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Graph_Convolutional_Policy_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goal-directed graph generation through reinforcement Learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://arxiv.org/abs/1806.02473</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/GroupNorm_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/GroupNorm_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Normalization_Layer"/>
        <obo:IAO_0000115>Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>GroupNorm</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.GroupNorm</oboInOwl:hasExactSynonym>
        <rdfs:label>GroupNorm Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/GroupNorm_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Group_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Group_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Human_Bias"/>
        <obo:IAO_0000115>A pattern of favoring members of one&apos;s in-group over out-group members. This can be expressed in evaluation of others, in allocation of resources, and in many other ways.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>In-group Favoritism</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>In-group bias</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>In-group preference</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>In-group–out-group Bias</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Intergroup bias</oboInOwl:hasExactSynonym>
        <rdfs:label>Group Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Group_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A pattern of favoring members of one&apos;s in-group over out-group members. This can be expressed in evaluation of others, in allocation of resources, and in many other ways.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/In-group_favoritism</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Groupthink_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Groupthink_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Group_Bias"/>
        <obo:IAO_0000115>A psychological phenomenon that occurs when people in a group tend to make non-optimal decisions based on their desire to conform to the group, or fear of dissenting with the group. In groupthink, individuals often refrain from expressing their personal disagreement with the group, hesitating to voice opinions that do not align with the group.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Groupthink</oboInOwl:hasExactSynonym>
        <rdfs:label>Groupthink Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Groupthink_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A psychological phenomenon that occurs when people in a group tend to make non-optimal decisions based on their desire to conform to the group, or fear of dissenting with the group. In groupthink, individuals often refrain from expressing their personal disagreement with the group, hesitating to voice opinions that do not align with the group.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Hard_Sigmoid_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/Hard_Sigmoid_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>A faster approximation of the sigmoid activation. Piecewise linear approximation of the sigmoid function. Ref: &apos;https://en.wikipedia.org/wiki/Hard_sigmoid&apos;</obo:IAO_0000115>
        <rdfs:label>Hard Sigmoid Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Hard_Sigmoid_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A faster approximation of the sigmoid activation. Piecewise linear approximation of the sigmoid function. Ref: &apos;https://en.wikipedia.org/wiki/Hard_sigmoid&apos;</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/hard_sigmoid</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Hashing_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Hashing_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Categorical_Features_Preprocessing_Layer"/>
        <obo:IAO_0000115>A preprocessing layer which hashes and bins categorical features. This layer transforms categorical inputs to hashed output. It element-wise converts a ints or strings to ints in a fixed range. The stable hash function uses tensorflow::ops::Fingerprint to produce the same output consistently across all platforms. This layer uses FarmHash64 by default, which provides a consistent hashed output across different platforms and is stable across invocations, regardless of device and context, by mixing the input bits thoroughly. If you want to obfuscate the hashed output, you can also pass a random salt argument in the constructor. In that case, the layer will use the SipHash64 hash function, with the salt value serving as additional input to the hash function.</obo:IAO_0000115>
        <rdfs:label>Hashing Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Hashing_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which hashes and bins categorical features. This layer transforms categorical inputs to hashed output. It element-wise converts a ints or strings to ints in a fixed range. The stable hash function uses tensorflow::ops::Fingerprint to produce the same output consistently across all platforms. This layer uses FarmHash64 by default, which provides a consistent hashed output across different platforms and is stable across invocations, regardless of device and context, by mixing the input bits thoroughly. If you want to obfuscate the hashed output, you can also pass a random salt argument in the constructor. In that case, the layer will use the SipHash64 hash function, with the salt value serving as additional input to the hash function.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Hashing</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Hidden_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Hidden_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A hidden layer is located between the input and output of the algorithm, in which the function applies weights to the inputs and directs them through an activation function as the output. In short, the hidden layers perform nonlinear transformations of the inputs entered into the network. Hidden layers vary depending on the function of the neural network, and similarly, the layers may vary depending on their associated weights.</obo:IAO_0000115>
        <rdfs:label>Hidden Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Hidden_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A hidden layer is located between the input and output of the algorithm, in which the function applies weights to the inputs and directs them through an activation function as the output. In short, the hidden layers perform nonlinear transformations of the inputs entered into the network. Hidden layers vary depending on the function of the neural network, and similarly, the layers may vary depending on their associated weights.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://deepai.org/machine-Learning-glossary-and-terms/hidden-layer-machine-Learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Hierarchical_Classification -->

    <owl:Class rdf:about="https://w3id.org/aio/Hierarchical_Classification">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Classification"/>
        <obo:IAO_0000115>Methods that group things according to a hierarchy.</obo:IAO_0000115>
        <rdfs:label>Hierarchical Classification</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Hierarchical_Classification"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that group things according to a hierarchy.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Hierarchical_classification</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Hierarchical_Clustering -->

    <owl:Class rdf:about="https://w3id.org/aio/Hierarchical_Clustering">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Clustering"/>
        <obo:IAO_0000115>Methods that seek to build a hierarchy of clusters.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>HCL</oboInOwl:hasExactSynonym>
        <rdfs:label>Hierarchical Clustering</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Hierarchical_Clustering"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that seek to build a hierarchy of clusters.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Hierarchical_clustering</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Historical_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Historical_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Bias"/>
        <obo:IAO_0000115>Referring to the long-standing biases encoded in society over time. Related to, but distinct from, biases in historical description, or the interpretation, analysis, and explanation of history. A common example of historical bias is the tendency to view the larger world from a Western or European view.</obo:IAO_0000115>
        <rdfs:label>Historical Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Historical_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Referring to the long-standing biases encoded in society over time. Related to, but distinct from, biases in historical description, or the interpretation, analysis, and explanation of history. A common example of historical bias is the tendency to view the larger world from a Western or European view.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Hopfield_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Hopfield_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/SCN"/>
        <obo:IAO_0000115>A Hopfield network is a form of recurrent artificial neural network and a type of spin glass system popularised by John Hopfield in 1982 as described earlier by Little in 1974 based on Ernst Ising&apos;s work with Wilhelm Lenz on the Ising model. Hopfield networks serve as content-addressable (&quot;associative&quot;) memory systems with binary threshold nodes, or with continuous variables. Hopfield networks also provide a model for understanding human memory. (https://en.wikipedia.org/wiki/Hopfield_network)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>HN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Ising model of a neural network</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Ising–Lenz–Little model</oboInOwl:hasExactSynonym>
        <rdfs:comment>Backfed input</rdfs:comment>
        <rdfs:label>Hopfield Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Hopfield_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A Hopfield network is a form of recurrent artificial neural network and a type of spin glass system popularised by John Hopfield in 1982 as described earlier by Little in 1974 based on Ernst Ising&apos;s work with Wilhelm Lenz on the Ising model. Hopfield networks serve as content-addressable (&quot;associative&quot;) memory systems with binary threshold nodes, or with continuous variables. Hopfield networks also provide a model for understanding human memory. (https://en.wikipedia.org/wiki/Hopfield_network)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Hopfield_network</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Hostile_Attribution_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Hostile_Attribution_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Use_And_Interpretation_Bias"/>
        <obo:IAO_0000115>A bias wherein individuals perceive benign or ambiguous behaviors as hostile.</obo:IAO_0000115>
        <rdfs:label>Hostile Attribution Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Hostile_Attribution_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A bias wherein individuals perceive benign or ambiguous behaviors as hostile.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Interpretive_bias</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Human_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Human_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Bias"/>
        <obo:IAO_0000115>Systematic errors in human thought based on a limited number of heuristic principles and predicting values to simpler judgmental operations.</obo:IAO_0000115>
        <rdfs:label>Human Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Human_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Systematic errors in human thought based on a limited number of heuristic principles and predicting values to simpler judgmental operations.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Human_Reporting_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Human_Reporting_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>When users rely on automation as a heuristic replacement for their own information seeking and processing.</obo:IAO_0000115>
        <rdfs:label>Human Reporting Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Human_Reporting_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>When users rely on automation as a heuristic replacement for their own information seeking and processing.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Image_Augmentation_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Image_Augmentation_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A layer that performs image data preprocessing augmentations.</obo:IAO_0000115>
        <rdfs:label>Image Augmentation Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Image_Augmentation_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A layer that performs image data preprocessing augmentations.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://keras.io/guides/preprocessing_layers/</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Image_Preprocessing_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Image_Preprocessing_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A layer that performs image data preprocessing operations.</obo:IAO_0000115>
        <rdfs:label>Image Preprocessing Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Image_Preprocessing_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A layer that performs image data preprocessing operations.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://keras.io/guides/preprocessing_layers/</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Implicit_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Implicit_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>An unconscious belief, attitude, feeling, association, or stereotype that can affect the way in which humans process information, make decisions, and take actions.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Confirmatory Bias</oboInOwl:hasExactSynonym>
        <rdfs:label>Implicit Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Implicit_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>An unconscious belief, attitude, feeling, association, or stereotype that can affect the way in which humans process information, make decisions, and take actions.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Incremenetal_Few-shot_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Incremenetal_Few-shot_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>Methods that train a network on a base set of classes and then is presented several novel classes, each with only a few labeled examples.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>IFSL</oboInOwl:hasExactSynonym>
        <rdfs:label>Incremenetal Few-shot Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Incremenetal_Few-shot_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that train a network on a base set of classes and then is presented several novel classes, each with only a few labeled examples.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://arxiv.org/abs/1810.07218</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Individual_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Individual_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Bias"/>
        <obo:IAO_0000115>Individual bias is a persistent point of view or limited list of such points of view that one applies (&quot;parent&quot;, &quot;academic&quot;, &quot;professional&quot;, or etc.).</obo:IAO_0000115>
        <rdfs:label>Individual Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Individual bias is a persistent point of view or limited list of such points of view that one applies (&quot;parent&quot;, &quot;academic&quot;, &quot;professional&quot;, or etc.).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://develop.consumerium.org/wiki/Individual_bias</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Inherited_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Inherited_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Processing_Bias"/>
        <obo:IAO_0000115>Arises when applications that are built with machine Learning are used to generate inputs for other machine Learning algorithms. If the output is biased in any way, this bias may be inherited by systems using the output as input to learn other models.</obo:IAO_0000115>
        <rdfs:label>Inherited Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Inherited_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises when applications that are built with machine Learning are used to generate inputs for other machine Learning algorithms. If the output is biased in any way, this bias may be inherited by systems using the output as input to learn other models.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/InputLayer_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/InputLayer_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Layer to be used as an entry point into a Network (a graph of layers).</obo:IAO_0000115>
        <rdfs:label>InputLayer Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/InputLayer_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer to be used as an entry point into a Network (a graph of layers).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputLayer</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/InputSpec_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/InputSpec_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Specifies the rank, dtype and shape of every input to a layer. Layers can expose (if appropriate) an input_spec attribute: an instance of InputSpec, or a nested structure of InputSpec instances (one per input tensor). These objects enable the layer to run input compatibility checks for input structure, input rank, input shape, and input dtype. A None entry in a shape is compatible with any dimension, a None shape is compatible with any shape.</obo:IAO_0000115>
        <rdfs:label>InputSpec Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/InputSpec_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Specifies the rank, dtype and shape of every input to a layer. Layers can expose (if appropriate) an input_spec attribute: an instance of InputSpec, or a nested structure of InputSpec instances (one per input tensor). These objects enable the layer to run input compatibility checks for input structure, input rank, input shape, and input dtype. A None entry in a shape is compatible with any dimension, a None shape is compatible with any shape.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputSpec</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Input_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Input_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>The input layer of a neural network is composed of artificial input neurons, and brings the initial data into the system for further processing by subsequent layers of artificial neurons. The input layer is the very beginning of the workflow for the artificial neural network.</obo:IAO_0000115>
        <rdfs:label>Input Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Input_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The input layer of a neural network is composed of artificial input neurons, and brings the initial data into the system for further processing by subsequent layers of artificial neurons. The input layer is the very beginning of the workflow for the artificial neural network.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.techopedia.com/definition/33262/input-layer-neural-networks#:~:text=Explains%20Input%20Layer-,What%20Does%20Input%20Layer%20Mean%3F,for%20the%20artificial%20neural%20network.</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/InstanceNorm1d_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/InstanceNorm1d_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Normalization_Layer"/>
        <obo:IAO_0000115>Applies Instance Normalization over a 2D (unbatched) or 3D (batched) input as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>InstanceNorm1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>InstanceNorm1d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.InstanceNorm1d</oboInOwl:hasExactSynonym>
        <rdfs:label>InstanceNorm1d Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/InstanceNorm1d_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies Instance Normalization over a 2D (unbatched) or 3D (batched) input as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/InstanceNorm2d -->

    <owl:Class rdf:about="https://w3id.org/aio/InstanceNorm2d">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Normalization_Layer"/>
        <obo:IAO_0000115>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>InstanceNorm2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>InstanceNorm2d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.InstanceNorm2d</oboInOwl:hasExactSynonym>
        <rdfs:label>InstanceNorm2d</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/InstanceNorm2d"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/InstanceNorm3d_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/InstanceNorm3d_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Normalization_Layer"/>
        <obo:IAO_0000115>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>InstanceNorm3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>InstanceNorm3d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.InstanceNorm3d</oboInOwl:hasExactSynonym>
        <rdfs:label>InstanceNorm3d Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/InstanceNorm3d_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Institutional_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Institutional_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Bias"/>
        <obo:IAO_0000115>In contrast to biases exhibited at the level of individual persons, institutional bias refers to a tendency exhibited at the level of entire institutions, where practices or norms result in the favoring or disadvantaging of certain social groups. Common examples include institutional racism and institutional sexism.</obo:IAO_0000115>
        <rdfs:label>Institutional Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Institutional_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>In contrast to biases exhibited at the level of individual persons, institutional bias refers to a tendency exhibited at the level of entire institutions, where practices or norms result in the favoring or disadvantaging of certain social groups. Common examples include institutional racism and institutional sexism.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/IntegerLookup_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/IntegerLookup_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Categorical_Features_Preprocessing_Layer"/>
        <obo:IAO_0000115>A preprocessing layer which maps integer features to contiguous ranges.</obo:IAO_0000115>
        <rdfs:label>IntegerLookup Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/IntegerLookup_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which maps integer features to contiguous ranges.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/IntegerLookup</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Interpretation_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Interpretation_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>A form of information processing bias that can occur when users interpret algorithmic outputs according to their internalized biases and views.</obo:IAO_0000115>
        <rdfs:label>Interpretation Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Interpretation_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A form of information processing bias that can occur when users interpret algorithmic outputs according to their internalized biases and views.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/K-nearest_Neighbor_Algorithm -->

    <owl:Class rdf:about="https://w3id.org/aio/K-nearest_Neighbor_Algorithm">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>An algorithm to group objects by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>K-NN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>KNN</oboInOwl:hasExactSynonym>
        <rdfs:label>K-nearest Neighbor Algorithm</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/K-nearest_Neighbor_Algorithm"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>An algorithm to group objects by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/K-nearest_Neighbor_Classification_Algorithm -->

    <owl:Class rdf:about="https://w3id.org/aio/K-nearest_Neighbor_Classification_Algorithm">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Classification"/>
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Clustering"/>
        <obo:IAO_0000115>An algorithm to classify objects by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>K-NN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>KNN</oboInOwl:hasExactSynonym>
        <rdfs:label>K-nearest Neighbor Classification Algorithm</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/K-nearest_Neighbor_Classification_Algorithm"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>An algorithm to classify objects by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/K-nearest_Neighbor_Regression_Algorithm -->

    <owl:Class rdf:about="https://w3id.org/aio/K-nearest_Neighbor_Regression_Algorithm">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regression_Analysis"/>
        <obo:IAO_0000115>An algorithm to assign the average of the values of k nearest neighbors to objects.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>K-NN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>KNN</oboInOwl:hasExactSynonym>
        <rdfs:label>K-nearest Neighbor Regression Algorithm</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/K-nearest_Neighbor_Regression_Algorithm"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>An algorithm to assign the average of the values of k nearest neighbors to objects.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Kohonen_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Kohonen_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Network"/>
        <obo:IAO_0000115>A self-organizing map (SOM) or self-organizing feature map (SOFM) is an unsupervised machine Learning technique used to produce a low-dimensional (typically two-dimensional) representation of a higher dimensional data set while preserving the topological structure of the data. For example, a data set with p variables measured in n observations could be represented as clusters of observations with similar values for the variables. These clusters then could be visualized as a two-dimensional &quot;map&quot; such that observations in proximal clusters have more similar values than observations in distal clusters. This can make high-dimensional data easier to visualize and analyze. An SOM is a type of artificial neural network but is trained using competitive Learning rather than the error-correction Learning (e.g., backpropagation with gradient descent) used by other artificial neural networks. The SOM was introduced by the Finnish professor Teuvo Kohonen in the 1980s and therefore is sometimes called a Kohonen map or Kohonen network.[1][2] The Kohonen map or network is a computationally convenient abstraction building on biological models of neural systems from the 1970s[3] and morphogenesis models dating back to Alan Turing in the 1950s.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>KN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>SOFM</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>SOM</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Self-Organizing Feature Map</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Self-Organizing Map</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Hidden</rdfs:comment>
        <rdfs:label>Kohonen Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Kohonen_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A self-organizing map (SOM) or self-organizing feature map (SOFM) is an unsupervised machine Learning technique used to produce a low-dimensional (typically two-dimensional) representation of a higher dimensional data set while preserving the topological structure of the data. For example, a data set with p variables measured in n observations could be represented as clusters of observations with similar values for the variables. These clusters then could be visualized as a two-dimensional &quot;map&quot; such that observations in proximal clusters have more similar values than observations in distal clusters. This can make high-dimensional data easier to visualize and analyze. An SOM is a type of artificial neural network but is trained using competitive Learning rather than the error-correction Learning (e.g., backpropagation with gradient descent) used by other artificial neural networks. The SOM was introduced by the Finnish professor Teuvo Kohonen in the 1980s and therefore is sometimes called a Kohonen map or Kohonen network.[1][2] The Kohonen map or network is a computationally convenient abstraction building on biological models of neural systems from the 1970s[3] and morphogenesis models dating back to Alan Turing in the 1950s.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Self-organizing_map</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LLM -->

    <owl:Class rdf:about="https://w3id.org/aio/LLM"/>
    


    <!-- https://w3id.org/aio/LPPool1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LPPool1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 1D power-average pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LPPool1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>LPPool1d</oboInOwl:hasExactSynonym>
        <rdfs:label>LPPool1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LPPool1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 1D power-average pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LPPool2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LPPool2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Applies a 2D power-average pooling over an input signal composed of several input planes.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LPPool2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>LPPool2d</oboInOwl:hasExactSynonym>
        <rdfs:label>LPPool2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LPPool2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies a 2D power-average pooling over an input signal composed of several input planes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LSTM -->

    <owl:Class rdf:about="https://w3id.org/aio/LSTM"/>
    


    <!-- https://w3id.org/aio/LSTMCell_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LSTMCell_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Cell class for the LSTM layer.</obo:IAO_0000115>
        <rdfs:label>LSTMCell Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LSTMCell_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Cell class for the LSTM layer.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LSTM_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LSTM_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Recurrent_Layer"/>
        <obo:IAO_0000115>Long Short-Term Memory layer - Hochreiter 1997. Based on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or pure-TensorFlow) to maximize the performance. If a GPU is available and all the arguments to the layer meet the requirement of the cuDNN kernel (see below for details), the layer will use a fast cuDNN implementation. The requirements to use the cuDNN implementation are: 1. activation == tanh, 2. recurrent_activation == sigmoid, 3. recurrent_dropout == 0, 4. unroll is False, 5. use_bias is True, 6. Inputs, if use masking, are strictly right-padded, 7. Eager execution is enabled in the outermost context.</obo:IAO_0000115>
        <rdfs:label>LSTM Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LSTM_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Long Short-Term Memory layer - Hochreiter 1997. Based on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or pure-TensorFlow) to maximize the performance. If a GPU is available and all the arguments to the layer meet the requirement of the cuDNN kernel (see below for details), the layer will use a fast cuDNN implementation. The requirements to use the cuDNN implementation are: 1. activation == tanh, 2. recurrent_activation == sigmoid, 3. recurrent_dropout == 0, 4. unroll is False, 5. use_bias is True, 6. Inputs, if use masking, are strictly right-padded, 7. Eager execution is enabled in the outermost context.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Lambda_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Lambda_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Wraps arbitrary expressions as a Layer object.</obo:IAO_0000115>
        <rdfs:label>Lambda Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Lambda_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Wraps arbitrary expressions as a Layer object.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Large_Language_Model -->

    <owl:Class rdf:about="https://w3id.org/aio/Large_Language_Model">
        <obo:IAO_0000115>A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabeled text using self-supervised learning or semi-supervised learning.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LLM</oboInOwl:hasExactSynonym>
        <rdfs:label>Large Language Model</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Large_Language_Model"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabeled text using self-supervised learning or semi-supervised learning.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Large_language_model</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Lasso_Regression -->

    <owl:Class rdf:about="https://w3id.org/aio/Lasso_Regression">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regression_Analysis"/>
        <obo:IAO_0000115>A regression analysis method that performs both variable selection and regularizationin order to enhance the prediction accuracy and interpretability of the resulting statistical model.</obo:IAO_0000115>
        <rdfs:label>Lasso Regression</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Lasso_Regression"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A regression analysis method that performs both variable selection and regularizationin order to enhance the prediction accuracy and interpretability of the resulting statistical model.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Lasso_(statistics)</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Layer">
        <obo:IAO_0000115>Network layer parent class</obo:IAO_0000115>
        <rdfs:label>Layer</rdfs:label>
    </owl:Class>
    


    <!-- https://w3id.org/aio/LayerNorm_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LayerNorm_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Normalization_Layer"/>
        <obo:IAO_0000115>Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LayerNorm</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.LayerNorm</oboInOwl:hasExactSynonym>
        <rdfs:label>LayerNorm Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LayerNorm_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LayerNormalization_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LayerNormalization_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Normalization_Layer"/>
        <obo:IAO_0000115>Layer normalization layer (Ba et al., 2016). Normalize the activations of the previous layer for each given example in a batch independently, rather than across a batch like Batch Normalization. i.e. applies a transformation that maintains the mean activation within each example close to 0 and the activation standard deviation close to 1. Given a tensor inputs, moments are calculated and normalization is performed across the axes specified in axis.</obo:IAO_0000115>
        <rdfs:label>LayerNormalization Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LayerNormalization_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer normalization layer (Ba et al., 2016). Normalize the activations of the previous layer for each given example in a batch independently, rather than across a batch like Batch Normalization. i.e. applies a transformation that maintains the mean activation within each example close to 0 and the activation standard deviation close to 1. Given a tensor inputs, moments are calculated and normalization is performed across the axes specified in axis.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Layer_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Layer_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>This is the class from which all layers inherit. A layer is a callable object that takes as input one or more tensors and that outputs one or more tensors. It involves computation, defined in the call() method, and a state (weight variables). State can be created in various places, at the convenience of the subclass implementer: in __init__(); in the optional build() method, which is invoked by the first __call__() to the layer, and supplies the shape(s) of the input(s), which may not have been known at initialization time; in the first invocation of call(), with some caveats discussed below. Users will just instantiate a layer and then treat it as a callable.</obo:IAO_0000115>
        <rdfs:label>Layer Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Layer_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>This is the class from which all layers inherit. A layer is a callable object that takes as input one or more tensors and that outputs one or more tensors. It involves computation, defined in the call() method, and a state (weight variables). State can be created in various places, at the convenience of the subclass implementer: in __init__(); in the optional build() method, which is invoked by the first __call__() to the layer, and supplies the shape(s) of the input(s), which may not have been known at initialization time; in the first invocation of call(), with some caveats discussed below. Users will just instantiate a layer and then treat it as a callable.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LazyBatchNorm1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LazyBatchNorm1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/BatchNormalization_Layer"/>
        <obo:IAO_0000115>A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size(1).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LazyBatchNorm1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>LazyBatchNorm1d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.LazyBatchNorm1d</oboInOwl:hasExactSynonym>
        <rdfs:label>LazyBatchNorm1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LazyBatchNorm1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size(1).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LazyBatchNorm2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LazyBatchNorm2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/BatchNormalization_Layer"/>
        <obo:IAO_0000115>A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size(1).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LazyBatchNorm2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>LazyBatchNorm2d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.LazyBatchNorm2d</oboInOwl:hasExactSynonym>
        <rdfs:label>LazyBatchNorm2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LazyBatchNorm2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size(1).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LazyBatchNorm3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LazyBatchNorm3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/BatchNormalization_Layer"/>
        <obo:IAO_0000115>A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size(1).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LazyBatchNorm3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>LazyBatchNorm3d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.LazyBatchNorm3d</oboInOwl:hasExactSynonym>
        <rdfs:label>LazyBatchNorm3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LazyBatchNorm3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size(1).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LazyInstanceNorm1d_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LazyInstanceNorm1d_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Normalization_Layer"/>
        <obo:IAO_0000115>A torch.nn.InstanceNorm1d module with lazy initialization of the num_features argument of the InstanceNorm1d that is inferred from the input.size(1).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LazyInstanceNorm1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>LazyInstanceNorm1d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.LazyInstanceNorm1d</oboInOwl:hasExactSynonym>
        <rdfs:label>LazyInstanceNorm1d Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LazyInstanceNorm1d_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A torch.nn.InstanceNorm1d module with lazy initialization of the num_features argument of the InstanceNorm1d that is inferred from the input.size(1).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LazyInstanceNorm2d_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LazyInstanceNorm2d_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Normalization_Layer"/>
        <obo:IAO_0000115>A torch.nn.InstanceNorm2d module with lazy initialization of the num_features argument of the InstanceNorm2d that is inferred from the input.size(1).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LazyInstanceNorm2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>LazyInstanceNorm2d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.LazyInstanceNorm2d</oboInOwl:hasExactSynonym>
        <rdfs:label>LazyInstanceNorm2d Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LazyInstanceNorm2d_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A torch.nn.InstanceNorm2d module with lazy initialization of the num_features argument of the InstanceNorm2d that is inferred from the input.size(1).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LazyInstanceNorm3d_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LazyInstanceNorm3d_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Normalization_Layer"/>
        <obo:IAO_0000115>A torch.nn.InstanceNorm3d module with lazy initialization of the num_features argument of the InstanceNorm3d that is inferred from the input.size(1).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LazyInstanceNorm3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>LazyInstanceNorm3d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.LazyInstanceNorm3d</oboInOwl:hasExactSynonym>
        <rdfs:label>LazyInstanceNorm3d Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LazyInstanceNorm3d_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A torch.nn.InstanceNorm3d module with lazy initialization of the num_features argument of the InstanceNorm3d that is inferred from the input.size(1).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LeakyReLU_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LeakyReLU_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Activation_Layer"/>
        <obo:IAO_0000115>Leaky version of a Rectified Linear Unit.</obo:IAO_0000115>
        <rdfs:label>LeakyReLU Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LeakyReLU_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Leaky version of a Rectified Linear Unit.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Least-squares_Analysis -->

    <owl:Class rdf:about="https://w3id.org/aio/Least-squares_Analysis">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regression_Analysis"/>
        <obo:IAO_0000115>A standard approach in regression analysis to approximate the solution of overdetermined systems(sets of equations in which there are more equations than unknowns) by minimizing the sum of the squares of the residuals (a residual being the difference between an observed value and the fitted value provided by a model) made in the results of each individual equation.</obo:IAO_0000115>
        <rdfs:label>Least-squares Analysis</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Least-squares_Analysis"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A standard approach in regression analysis to approximate the solution of overdetermined systems(sets of equations in which there are more equations than unknowns) by minimizing the sum of the squares of the residuals (a residual being the difference between an observed value and the fitted value provided by a model) made in the results of each individual equation.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Least_squares</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Linear_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/Linear_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>A linear function has the form f(x) = a + bx.</obo:IAO_0000115>
        <rdfs:label>Linear Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Linear_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A linear function has the form f(x) = a + bx.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Linear_Regression -->

    <owl:Class rdf:about="https://w3id.org/aio/Linear_Regression">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regression_Analysis"/>
        <obo:IAO_0000115>A linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables).</obo:IAO_0000115>
        <rdfs:label>Linear Regression</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Linear_Regression"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Linear_regression</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Linking_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Linking_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Use_And_Interpretation_Bias"/>
        <obo:IAO_0000115>Arises when network attributes obtained from user connections, activities, or interactions differ and misrepresent the true behavior of the users.</obo:IAO_0000115>
        <rdfs:label>Linking Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Linking_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises when network attributes obtained from user connections, activities, or interactions differ and misrepresent the true behavior of the users.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Liquid_State_Machine_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Liquid_State_Machine_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Network"/>
        <obo:IAO_0000115>A liquid state machine (LSM) is a type of reservoir computer that uses a spiking neural network. An LSM consists of a large collection of units (called nodes, or neurons). Each node receives time varying input from external sources (the inputs) as well as from other nodes. Nodes are randomly connected to each other. The recurrent nature of the connections turns the time varying input into a spatio-temporal pattern of activations in the network nodes. The spatio-temporal patterns of activation are read out by linear discriminant units. The soup of recurrently connected nodes will end up computing a large variety of nonlinear functions on the input. Given a large enough variety of such nonlinear functions, it is theoretically possible to obtain linear combinations (using the read out units) to perform whatever mathematical operation is needed to perform a certain task, such as speech recognition or computer vision. The word liquid in the name comes from the analogy drawn to dropping a stone into a still body of water or other liquid. The falling stone will generate ripples in the liquid. The input (motion of the falling stone) has been converted into a spatio-temporal pattern of liquid displacement (ripples). (https://en.wikipedia.org/wiki/Liquid_state_machine)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LSM</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Spiking Hidden, Output</rdfs:comment>
        <rdfs:label>Liquid State Machine Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Liquid_State_Machine_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A liquid state machine (LSM) is a type of reservoir computer that uses a spiking neural network. An LSM consists of a large collection of units (called nodes, or neurons). Each node receives time varying input from external sources (the inputs) as well as from other nodes. Nodes are randomly connected to each other. The recurrent nature of the connections turns the time varying input into a spatio-temporal pattern of activations in the network nodes. The spatio-temporal patterns of activation are read out by linear discriminant units. The soup of recurrently connected nodes will end up computing a large variety of nonlinear functions on the input. Given a large enough variety of such nonlinear functions, it is theoretically possible to obtain linear combinations (using the read out units) to perform whatever mathematical operation is needed to perform a certain task, such as speech recognition or computer vision. The word liquid in the name comes from the analogy drawn to dropping a stone into a still body of water or other liquid. The falling stone will generate ripples in the liquid. The input (motion of the falling stone) has been converted into a spatio-temporal pattern of liquid displacement (ripples). (https://en.wikipedia.org/wiki/Liquid_state_machine)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Liquid_state_machine</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LocalResponseNorm_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LocalResponseNorm_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Normalization_Layer"/>
        <obo:IAO_0000115>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LocalResponseNorm</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.LocalResponseNorm</oboInOwl:hasExactSynonym>
        <rdfs:label>LocalResponseNorm Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LocalResponseNorm_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Locally-connected_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Locally-connected_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>The LocallyConnected1D layer works similarly to the Convolution1D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.</obo:IAO_0000115>
        <rdfs:label>Locally-connected Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Locally-connected_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The LocallyConnected1D layer works similarly to the Convolution1D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://faroit.com/keras-docs/1.2.2/layers/local/</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LocallyConnected1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LocallyConnected1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Locally-connected_Layer"/>
        <obo:IAO_0000115>Locally-connected layer for 1D inputs. The LocallyConnected1D layer works similarly to the Conv1D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.</obo:IAO_0000115>
        <rdfs:label>LocallyConnected1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LocallyConnected1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Locally-connected layer for 1D inputs. The LocallyConnected1D layer works similarly to the Conv1D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/LocallyConnected2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/LocallyConnected2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Locally-connected_Layer"/>
        <obo:IAO_0000115>Locally-connected layer for 2D inputs. The LocallyConnected2D layer works similarly to the Conv2D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.</obo:IAO_0000115>
        <rdfs:label>LocallyConnected2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/LocallyConnected2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Locally-connected layer for 2D inputs. The LocallyConnected2D layer works similarly to the Conv2D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Logistic_Regression -->

    <owl:Class rdf:about="https://w3id.org/aio/Logistic_Regression">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regression_Analysis"/>
        <obo:IAO_0000115>A statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables.</obo:IAO_0000115>
        <rdfs:label>Logistic Regression</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Logistic_Regression"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Logistic_regression</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Long_Short_Term_Memory -->

    <owl:Class rdf:about="https://w3id.org/aio/Long_Short_Term_Memory">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/RecNN"/>
        <obo:IAO_0000115>Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep Learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can process not only single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDSs (intrusion detection systems). A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>LSTM</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Memory Cell, Output</rdfs:comment>
        <rdfs:label>Long Short Term Memory</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Long_Short_Term_Memory"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep Learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can process not only single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDSs (intrusion detection systems). A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Long_short-term_memory</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Loss_Of_Situational_Awareness_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Loss_Of_Situational_Awareness_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>When automation leads to humans being unaware of their situation such that, when control of a system is given back to them in a situation where humans and machines cooperate, they are unprepared to assume their duties. This can be a loss of awareness over what automation is and isn’t taking care of.</obo:IAO_0000115>
        <rdfs:label>Loss Of Situational Awareness Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Loss_Of_Situational_Awareness_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>When automation leads to humans being unaware of their situation such that, when control of a system is given back to them in a situation where humans and machines cooperate, they are unprepared to assume their duties. This can be a loss of awareness over what automation is and isn’t taking care of.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Machine_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Machine_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Method"/>
        <obo:IAO_0000115>A field of inquiry devoted to understanding and building methods that &apos;learn&apos;, that is, methods that leverage data to improve performance on some set of tasks.</obo:IAO_0000115>
        <rdfs:label>Machine Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A field of inquiry devoted to understanding and building methods that &apos;learn&apos;, that is, methods that leverage data to improve performance on some set of tasks.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Machine_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Manifold_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Manifold_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Dimensionality_Reduction"/>
        <obo:IAO_0000115>Methods based on the assumption that one&apos;s observed data lie on a low-dimensional manifold embedded in a higher-dimensional space.</obo:IAO_0000115>
        <rdfs:label>Manifold Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Manifold_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods based on the assumption that one&apos;s observed data lie on a low-dimensional manifold embedded in a higher-dimensional space.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://arxiv.org/abs/2011.01307</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Markov_Chain -->

    <owl:Class rdf:about="https://w3id.org/aio/Markov_Chain">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Network"/>
        <obo:IAO_0000115>A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.[1][2][3] A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a continuous-time Markov chain (CTMC). It is named after the Russian mathematician Andrey Markov.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>MC</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MP</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Markov Process</oboInOwl:hasExactSynonym>
        <rdfs:comment>Probalistic Hidden</rdfs:comment>
        <rdfs:label>Markov Chain</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Markov_Chain"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.[1][2][3] A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a continuous-time Markov chain (CTMC). It is named after the Russian mathematician Andrey Markov.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Markov_chain</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Masking_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Masking_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Masks a sequence by using a mask value to skip timesteps. For each timestep in the input tensor (dimension #1 in the tensor), if all values in the input tensor at that timestep are equal to mask_value, then the timestep will be masked (skipped) in all downstream layers (as long as they support masking). If any downstream layer does not support masking yet receives such an input mask, an exception will be raised.</obo:IAO_0000115>
        <rdfs:label>Masking Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Masking_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Masks a sequence by using a mask value to skip timesteps. For each timestep in the input tensor (dimension #1 in the tensor), if all values in the input tensor at that timestep are equal to mask_value, then the timestep will be masked (skipped) in all downstream layers (as long as they support masking). If any downstream layer does not support masking yet receives such an input mask, an exception will be raised.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/MaxPooling1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/MaxPooling1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Max pooling operation for 1D temporal data. Downsamples the input representation by taking the maximum value over a spatial window of size pool_size. The window is shifted by strides. The resulting output, when using the &quot;valid&quot; padding option, has a shape of: output_shape = (input_shape - pool_size + 1) / strides) The resulting output shape when using the &quot;same&quot; padding option is: output_shape = input_shape / strides.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>MaxPool1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MaxPool1d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MaxPooling1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MaxPooling1d</oboInOwl:hasExactSynonym>
        <rdfs:label>MaxPooling1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/MaxPooling1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Max pooling operation for 1D temporal data. Downsamples the input representation by taking the maximum value over a spatial window of size pool_size. The window is shifted by strides. The resulting output, when using the &quot;valid&quot; padding option, has a shape of: output_shape = (input_shape - pool_size + 1) / strides) The resulting output shape when using the &quot;same&quot; padding option is: output_shape = input_shape / strides.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/MaxPooling2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/MaxPooling2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Max pooling operation for 2D spatial data.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>MaxPool2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MaxPool2d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MaxPooling2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MaxPooling2d</oboInOwl:hasExactSynonym>
        <rdfs:label>MaxPooling2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/MaxPooling2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Max pooling operation for 2D spatial data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/MaxPooling3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/MaxPooling3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Max pooling operation for 3D data (spatial or spatio-temporal). Downsamples the input along its spatial dimensions (depth, height, and width) by taking the maximum value over an input window (of size defined by pool_size) for each channel of the input. The window is shifted by strides along each dimension.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>MaxPool3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MaxPool3d</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MaxPooling3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MaxPooling3d</oboInOwl:hasExactSynonym>
        <rdfs:label>MaxPooling3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/MaxPooling3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Max pooling operation for 3D data (spatial or spatio-temporal). Downsamples the input along its spatial dimensions (depth, height, and width) by taking the maximum value over an input window (of size defined by pool_size) for each channel of the input. The window is shifted by strides along each dimension.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool3D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/MaxUnpool1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/MaxUnpool1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Computes a partial inverse of MaxPool1d.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>MaxUnpool1D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MaxUnpool1d</oboInOwl:hasExactSynonym>
        <rdfs:label>MaxUnpool1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/MaxUnpool1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Computes a partial inverse of MaxPool1d.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/MaxUnpool2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/MaxUnpool2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Computes a partial inverse of MaxPool2d.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>MaxUnpool2D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MaxUnpool2d</oboInOwl:hasExactSynonym>
        <rdfs:label>MaxUnpool2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/MaxUnpool2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Computes a partial inverse of MaxPool2d.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/MaxUnpool3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/MaxUnpool3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <obo:IAO_0000115>Computes a partial inverse of MaxPool3d.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>MaxUnpool3D</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>MaxUnpool3d</oboInOwl:hasExactSynonym>
        <rdfs:label>MaxUnpool3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/MaxUnpool3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Computes a partial inverse of MaxPool3d.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#pooling-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Maximum_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Maximum_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Merging_Layer"/>
        <obo:IAO_0000115>Layer that computes the maximum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</obo:IAO_0000115>
        <rdfs:label>Maximum Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Maximum_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer that computes the maximum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Maximum</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Measurement_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Measurement_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <obo:IAO_0000115>Arises when features and labels are proxies for desired quantities, potentially leaving out important factors or introducing group or input-dependent noise that leads to differential performance.</obo:IAO_0000115>
        <rdfs:label>Measurement Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Measurement_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises when features and labels are proxies for desired quantities, potentially leaving out important factors or introducing group or input-dependent noise that leads to differential performance.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Merging_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Merging_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A layer used to merge a list of inputs.</obo:IAO_0000115>
        <rdfs:label>Merging Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Merging_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A layer used to merge a list of inputs.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tutorialspoint.com/keras/keras_merge_layer.htm</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Meta-Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Meta-Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Automatic learning algorithms applied to metadata about machine Learning experiments.</obo:IAO_0000115>
        <rdfs:label>Meta-Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Meta-Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Automatic learning algorithms applied to metadata about machine Learning experiments.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Meta_learning_(computer_science)</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Method -->

    <owl:Class rdf:about="https://w3id.org/aio/Method">
        <obo:IAO_0000115>Method parent class.</obo:IAO_0000115>
        <rdfs:label>Method</rdfs:label>
    </owl:Class>
    


    <!-- https://w3id.org/aio/Metric_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Metric_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>Methods which can learn a representation function that maps objects into an embedded space.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Distance Metric Learning</oboInOwl:hasExactSynonym>
        <rdfs:label>Metric Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Metric_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods which can learn a representation function that maps objects into an embedded space.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://paperswithcode.com/task/metric-learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Minimum_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Minimum_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Merging_Layer"/>
        <obo:IAO_0000115>Layer that computes the minimum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</obo:IAO_0000115>
        <rdfs:label>Minimum Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Minimum_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer that computes the minimum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Minimum</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Mode_Confusion_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Mode_Confusion_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>When modal interfaces confuse human operators, who misunderstand which mode the system is using, taking actions which are correct for a different mode but incorrect for their current situation. This is the cause of many deadly accidents, but also a source of confusion in everyday life.</obo:IAO_0000115>
        <rdfs:label>Mode Confusion Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Mode_Confusion_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>When modal interfaces confuse human operators, who misunderstand which mode the system is using, taking actions which are correct for a different mode but incorrect for their current situation. This is the cause of many deadly accidents, but also a source of confusion in everyday life.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Model_Selection_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Model_Selection_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Processing_Bias"/>
        <obo:IAO_0000115>The bias introduced while using the data to select a single seemingly “best” model from a large set of models employing many predictor variables. Model selection bias also occurs when an explanatory variable has a weak relationship with the response variable.</obo:IAO_0000115>
        <rdfs:label>Model Selection Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Model_Selection_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The bias introduced while using the data to select a single seemingly “best” model from a large set of models employing many predictor variables. Model selection bias also occurs when an explanatory variable has a weak relationship with the response variable.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/MultiHeadAttention_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/MultiHeadAttention_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Attention_Layer"/>
        <obo:IAO_0000115>MultiHeadAttention layer. This is an implementation of multi-headed attention as described in the paper &quot;Attention is all you Need&quot; (Vaswani et al., 2017). If query, key, value are the same, then this is self-attention. Each timestep in query attends to the corresponding sequence in key, and returns a fixed-width vector.This layer first projects query, key and value. These are (effectively) a list of tensors of length num_attention_heads, where the corresponding shapes are (batch_size, &lt;query dimensions&gt;, key_dim), (batch_size, &lt;key/value dimensions&gt;, key_dim), (batch_size, &lt;key/value dimensions&gt;, value_dim).Then, the query and key tensors are dot-producted and scaled. These are softmaxed to obtain attention probabilities. The value tensors are then interpolated by these probabilities, then concatenated back to a single tensor. Finally, the result tensor with the last dimension as value_dim can take an linear projection and return. When using MultiHeadAttention inside a custom Layer, the custom Layer must implement build() and call MultiHeadAttention&apos;s _build_from_signature(). This enables weights to be restored correctly when the model is loaded.</obo:IAO_0000115>
        <rdfs:label>MultiHeadAttention Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/MultiHeadAttention_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>MultiHeadAttention layer. This is an implementation of multi-headed attention as described in the paper &quot;Attention is all you Need&quot; (Vaswani et al., 2017). If query, key, value are the same, then this is self-attention. Each timestep in query attends to the corresponding sequence in key, and returns a fixed-width vector.This layer first projects query, key and value. These are (effectively) a list of tensors of length num_attention_heads, where the corresponding shapes are (batch_size, &lt;query dimensions&gt;, key_dim), (batch_size, &lt;key/value dimensions&gt;, key_dim), (batch_size, &lt;key/value dimensions&gt;, value_dim).Then, the query and key tensors are dot-producted and scaled. These are softmaxed to obtain attention probabilities. The value tensors are then interpolated by these probabilities, then concatenated back to a single tensor. Finally, the result tensor with the last dimension as value_dim can take an linear projection and return. When using MultiHeadAttention inside a custom Layer, the custom Layer must implement build() and call MultiHeadAttention&apos;s _build_from_signature(). This enables weights to be restored correctly when the model is loaded.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Multiclass_Classification -->

    <owl:Class rdf:about="https://w3id.org/aio/Multiclass_Classification">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Classification"/>
        <obo:IAO_0000115>Methods that lassify instances into one of three or more classes (classifying instances into one of two classes is called binary classification).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Multinomial Classification</oboInOwl:hasExactSynonym>
        <rdfs:label>Multiclass Classification</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Multiclass_Classification"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that lassify instances into one of three or more classes (classifying instances into one of two classes is called binary classification).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Multiclass_classification</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Multidimensional_Scaling -->

    <owl:Class rdf:about="https://w3id.org/aio/Multidimensional_Scaling">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Dimensionality_Reduction"/>
        <obo:IAO_0000115>A method that translates information about the pairwise distances among a set of objects or individuals into a configuration of points mapped into an abstract Cartesian space.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>MDS</oboInOwl:hasExactSynonym>
        <rdfs:label>Multidimensional Scaling</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Multidimensional_Scaling"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A method that translates information about the pairwise distances among a set of objects or individuals into a configuration of points mapped into an abstract Cartesian space.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Multidimensional_scaling</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Multimodal_Deep_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Multimodal_Deep_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>Methods which can create models that can process and link information using various modalities.</obo:IAO_0000115>
        <rdfs:label>Multimodal Deep Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Multimodal_Deep_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods which can create models that can process and link information using various modalities.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://arxiv.org/abs/2105.11087</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Multimodal_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Multimodal_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Methods which can represent the joint representations of different modalities.</obo:IAO_0000115>
        <rdfs:label>Multimodal Learning</rdfs:label>
    </owl:Class>
    


    <!-- https://w3id.org/aio/Multiply_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Multiply_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Merging_Layer"/>
        <obo:IAO_0000115>Layer that multiplies (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</obo:IAO_0000115>
        <rdfs:label>Multiply Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Multiply_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer that multiplies (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Multiply</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Natural_Language_Processing -->

    <owl:Class rdf:about="https://w3id.org/aio/Natural_Language_Processing">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>A subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>NLP</oboInOwl:hasExactSynonym>
        <rdfs:label>Natural Language Processing</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Natural_Language_Processing"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Natural_language_processing</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Network">
        <obo:IAO_0000115>Network parent class</obo:IAO_0000115>
        <rdfs:label>Network</rdfs:label>
    </owl:Class>
    


    <!-- https://w3id.org/aio/Neural_Turing_Machine_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Neural_Turing_Machine_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DFF"/>
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/LSTM"/>
        <obo:IAO_0000115>A Neural Turing machine (NTMs) is a recurrent neural network model. The approach was published by Alex Graves et al. in 2014. NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. An NTM has a neural network controller coupled to external memory resources, which it interacts with through attentional mechanisms. The memory interactions are differentiable end-to-end, making it possible to optimize them using gradient descent. An NTM with a long short-term memory (LSTM) network controller can infer simple algorithms such as copying, sorting, and associative recall from examples alone.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>NTM</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Hidden, Spiking Hidden, Output</rdfs:comment>
        <rdfs:label>Neural Turing Machine Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Neural_Turing_Machine_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A Neural Turing machine (NTMs) is a recurrent neural network model. The approach was published by Alex Graves et al. in 2014. NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. An NTM has a neural network controller coupled to external memory resources, which it interacts with through attentional mechanisms. The memory interactions are differentiable end-to-end, making it possible to optimize them using gradient descent. An NTM with a long short-term memory (LSTM) network controller can infer simple algorithms such as copying, sorting, and associative recall from examples alone.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Neural_Turing_machine</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Noise_Dense_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Noise_Dense_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Noisy dense layer that injects random noise to the weights of dense layer. Noisy dense layers are fully connected layers whose weights and biases are augmented by factorised Gaussian noise. The factorised Gaussian noise is controlled through gradient descent by a second weights layer. A NoisyDense layer implements the operation: $$ mathrm{NoisyDense}(x) = mathrm{activation}(mathrm{dot}(x, mu + (sigma cdot epsilon)) mathrm{bias}) $$ where mu is the standard weights layer, epsilon is the factorised Gaussian noise, and delta is a second weights layer which controls epsilon.</obo:IAO_0000115>
        <rdfs:label>Noise Dense Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Noise_Dense_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Noisy dense layer that injects random noise to the weights of dense layer. Noisy dense layers are fully connected layers whose weights and biases are augmented by factorised Gaussian noise. The factorised Gaussian noise is controlled through gradient descent by a second weights layer. A NoisyDense layer implements the operation: $$ mathrm{NoisyDense}(x) = mathrm{activation}(mathrm{dot}(x, mu + (sigma cdot epsilon)) mathrm{bias}) $$ where mu is the standard weights layer, epsilon is the factorised Gaussian noise, and delta is a second weights layer which controls epsilon.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/addons/api_docs/python/tfa/layers/NoisyDense</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Normalization_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Normalization_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Numerical_Features_Preprocessing_Layer"/>
        <obo:IAO_0000115>A preprocessing layer which normalizes continuous features.</obo:IAO_0000115>
        <rdfs:label>Normalization Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Normalization_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which normalizes continuous features.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Numerical_Features_Preprocessing_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Numerical_Features_Preprocessing_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A layer that performs numerical data preprocessing operations.</obo:IAO_0000115>
        <rdfs:label>Numerical Features Preprocessing Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Numerical_Features_Preprocessing_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A layer that performs numerical data preprocessing operations.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://keras.io/guides/preprocessing_layers/</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/One-shot_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/One-shot_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>A method which aims to classify objects from one, or only a few, examples.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>OSL</oboInOwl:hasExactSynonym>
        <rdfs:label>One-shot Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/One-shot_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A method which aims to classify objects from one, or only a few, examples.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/One-shot_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Output_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Output_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>The output layer in an artificial neural network is the last layer of neurons that produces given outputs for the program. Though they are made much like other artificial neurons in the neural network, output layer neurons may be built or observed in a different way, given that they are the last “actor” nodes on the network.</obo:IAO_0000115>
        <rdfs:label>Output Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Output_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The output layer in an artificial neural network is the last layer of neurons that produces given outputs for the program. Though they are made much like other artificial neurons in the neural network, output layer neurons may be built or observed in a different way, given that they are the last “actor” nodes on the network.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.techopedia.com/definition/33263/output-layer-neural-networks</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/PReLU_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/PReLU_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Activation_Layer"/>
        <obo:IAO_0000115>Parametric Rectified Linear Unit.</obo:IAO_0000115>
        <rdfs:label>PReLU Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/PReLU_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Parametric Rectified Linear Unit.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/PReLU</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Perceptron -->

    <owl:Class rdf:about="https://w3id.org/aio/Perceptron">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/ANN"/>
        <obo:IAO_0000115>The perceptron is an algorithm for supervised Learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. (https://en.wikipedia.org/wiki/Perceptron)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>SLP</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Single Layer Perceptron</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Output</rdfs:comment>
        <rdfs:label>Perceptron</rdfs:label>
    </owl:Class>
    


    <!-- https://w3id.org/aio/Permute_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Permute_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Reshaping_Layer"/>
        <obo:IAO_0000115>Permutes the dimensions of the input according to a given pattern. Useful e.g. connecting RNNs and convnets.</obo:IAO_0000115>
        <rdfs:label>Permute Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Permute_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Permutes the dimensions of the input according to a given pattern. Useful e.g. connecting RNNs and convnets.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Permute</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Pooling_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Pooling_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Pooling layers serve the dual purposes of mitigating the sensitivity of convolutional layers to location and of spatially downsampling representations.</obo:IAO_0000115>
        <rdfs:label>Pooling Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Pooling_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Pooling layers serve the dual purposes of mitigating the sensitivity of convolutional layers to location and of spatially downsampling representations.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://d2l.ai/chapter_convolutional-neural-networks/pooling.html</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Popularity_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Popularity_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <obo:IAO_0000115>A form of selection bias that occurs when items that are more popular are more exposed and less popular items are under-represented.</obo:IAO_0000115>
        <rdfs:label>Popularity Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Popularity_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A form of selection bias that occurs when items that are more popular are more exposed and less popular items are under-represented.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Population_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Population_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <obo:IAO_0000115>A form of selection bias that occurs when items that are more popular are more exposed and less popular items are under-represented.aSystematic distortions in demographics or other user characteristics between a population of users represented in a dataset or on a platform and some target population.</obo:IAO_0000115>
        <rdfs:label>Population Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Population_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A form of selection bias that occurs when items that are more popular are more exposed and less popular items are under-represented.aSystematic distortions in demographics or other user characteristics between a population of users represented in a dataset or on a platform and some target population.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Preprocessing_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Preprocessing_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A layer that performs data preprocessing operations.</obo:IAO_0000115>
        <rdfs:label>Preprocessing Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Preprocessing_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A layer that performs data preprocessing operations.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/guide/keras/preprocessing_layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Presentation_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Presentation_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>Biases arising from how information is presented on the Web, via a user interface, due to rating or ranking of output, or through users’ own self-selected, biased interaction.</obo:IAO_0000115>
        <rdfs:label>Presentation Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Presentation_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Biases arising from how information is presented on the Web, via a user interface, due to rating or ranking of output, or through users’ own self-selected, biased interaction.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Principal_Component_Analysis -->

    <owl:Class rdf:about="https://w3id.org/aio/Principal_Component_Analysis">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Dimensionality_Reduction"/>
        <obo:IAO_0000115>A method for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>PCA</oboInOwl:hasExactSynonym>
        <rdfs:label>Principal Component Analysis</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Principal_Component_Analysis"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A method for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Principal_component_analysis</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Probabilistic_Graphical_Model -->

    <owl:Class rdf:about="https://w3id.org/aio/Probabilistic_Graphical_Model">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>A probabilistic model for which a graph expresses the conditional dependence structure between random variables.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Graphical Model</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>PGM</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Structure Probabilistic Model</oboInOwl:hasExactSynonym>
        <rdfs:label>Probabilistic Graphical Model</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Probabilistic_Graphical_Model"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A probabilistic model for which a graph expresses the conditional dependence structure between random variables.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Graphical_model</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Probabilistic_Topic_Model -->

    <owl:Class rdf:about="https://w3id.org/aio/Probabilistic_Topic_Model">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Probabilistic_Graphical_Model"/>
        <obo:IAO_0000115>Methods that use statistical methods to analyze the words in each text to discover common themes, how those themes are connected to each other, and how they change over time.</obo:IAO_0000115>
        <rdfs:label>Probabilistic Topic Model</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Probabilistic_Topic_Model"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that use statistical methods to analyze the words in each text to discover common themes, how those themes are connected to each other, and how they change over time.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pyro.ai/examples/prodlda.html</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Processing_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Processing_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Computational_Bias"/>
        <obo:IAO_0000115>Judgement modulated by affect, which is influenced by the level of efficacy and efficiency in information processing; in cognitive sciences, processing bias is often referred to as an aesthetic judgement.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Validation Bias</oboInOwl:hasExactSynonym>
        <rdfs:label>Processing Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Processing_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Judgement modulated by affect, which is influenced by the level of efficacy and efficiency in information processing; in cognitive sciences, processing bias is often referred to as an aesthetic judgement.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://royalsocietypublishing.org/doi/10.1098/rspb.2019.0165#d1e5237</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Proportional_Hazards_Model -->

    <owl:Class rdf:about="https://w3id.org/aio/Proportional_Hazards_Model">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regression_Analysis"/>
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Survival_Analysis"/>
        <obo:IAO_0000115>A surival modeling method where the unique effect of a unit increase in a covariate is multiplicative with respect to the hazard rate.</obo:IAO_0000115>
        <rdfs:label>Proportional Hazards Model</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Proportional_Hazards_Model"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A surival modeling method where the unique effect of a unit increase in a covariate is multiplicative with respect to the hazard rate.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Proportional_hazards_modelProportional Hazards Model</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/RNN_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/RNN_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Recurrent_Layer"/>
        <obo:IAO_0000115>Base class for recurrent layers.</obo:IAO_0000115>
        <rdfs:label>RNN Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/RNN_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Base class for recurrent layers.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Radial_Basis_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Radial_Basis_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DFF"/>
        <obo:IAO_0000115>Like recurrent neural networks (RNNs), transformers are designed to handle sequential input data, such as natural language, for tasks such as translation and text summarization. However, unlike RNNs, transformers do not necessarily process the data in order. Rather, the attention mechanism provides context for any position in the input sequence.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>RBFN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>RBN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Radial Basis Function Network</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Hidden, Output</rdfs:comment>
        <rdfs:label>Radial Basis Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Radial_Basis_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Like recurrent neural networks (RNNs), transformers are designed to handle sequential input data, such as natural language, for tasks such as translation and text summarization. However, unlike RNNs, transformers do not necessarily process the data in order. Rather, the attention mechanism provides context for any position in the input sequence.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Radial_basis_function_network</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/RandomBrightness_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/RandomBrightness_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A preprocessing layer which randomly adjusts brightness during training. This layer will randomly increase/reduce the brightness for the input RGB images. At inference time, the output will be identical to the input. Call the layer with training=True to adjust the brightness of the input. Note that different brightness adjustment factors will be apply to each the images in the batch.</obo:IAO_0000115>
        <rdfs:label>RandomBrightness Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/RandomBrightness_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which randomly adjusts brightness during training. This layer will randomly increase/reduce the brightness for the input RGB images. At inference time, the output will be identical to the input. Call the layer with training=True to adjust the brightness of the input. Note that different brightness adjustment factors will be apply to each the images in the batch.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomBrightness</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/RandomContrast_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/RandomContrast_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A preprocessing layer which randomly adjusts contrast during training. This layer will randomly adjust the contrast of an image or images by a random factor. Contrast is adjusted independently for each channel of each image during training. For each channel, this layer computes the mean of the image pixels in the channel and then adjusts each component x of each pixel to (x - mean) * contrast_factor + mean. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and in integer or floating point dtype. By default, the layer will output floats. The output value will be clipped to the range [0, 255], the valid range of RGB colors.</obo:IAO_0000115>
        <rdfs:label>RandomContrast Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/RandomContrast_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which randomly adjusts contrast during training. This layer will randomly adjust the contrast of an image or images by a random factor. Contrast is adjusted independently for each channel of each image during training. For each channel, this layer computes the mean of the image pixels in the channel and then adjusts each component x of each pixel to (x - mean) * contrast_factor + mean. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and in integer or floating point dtype. By default, the layer will output floats. The output value will be clipped to the range [0, 255], the valid range of RGB colors.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomContrast</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/RandomCrop_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/RandomCrop_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A preprocessing layer which randomly crops images during training. During training, this layer will randomly choose a location to crop images down to a target size. The layer will crop all the images in the same batch to the same cropping location. At inference time, and during training if an input image is smaller than the target size, the input will be resized and cropped so as to return the largest possible window in the image that matches the target aspect ratio. If you need to apply random cropping at inference time, set training to True when calling the layer. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats.</obo:IAO_0000115>
        <rdfs:label>RandomCrop Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/RandomCrop_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which randomly crops images during training. During training, this layer will randomly choose a location to crop images down to a target size. The layer will crop all the images in the same batch to the same cropping location. At inference time, and during training if an input image is smaller than the target size, the input will be resized and cropped so as to return the largest possible window in the image that matches the target aspect ratio. If you need to apply random cropping at inference time, set training to True when calling the layer. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomCrop</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/RandomFlip_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/RandomFlip_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A preprocessing layer which randomly flips images during training. This layer will flip the images horizontally and or vertically based on the mode attribute. During inference time, the output will be identical to input. Call the layer with training=True to flip the input. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats.</obo:IAO_0000115>
        <rdfs:label>RandomFlip Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/RandomFlip_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which randomly flips images during training. This layer will flip the images horizontally and or vertically based on the mode attribute. During inference time, the output will be identical to input. Call the layer with training=True to flip the input. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomFlip</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/RandomHeight_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/RandomHeight_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A preprocessing layer which randomly varies image height during training. This layer adjusts the height of a batch of images by a random factor. The input should be a 3D (unbatched) or 4D (batched) tensor in the &quot;channels_last&quot; image data format. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats. By default, this layer is inactive during inference.</obo:IAO_0000115>
        <rdfs:label>RandomHeight Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/RandomHeight_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which randomly varies image height during training. This layer adjusts the height of a batch of images by a random factor. The input should be a 3D (unbatched) or 4D (batched) tensor in the &quot;channels_last&quot; image data format. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats. By default, this layer is inactive during inference.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomHeight</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/RandomRotation_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/RandomRotation_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A preprocessing layer which randomly rotates images during training.</obo:IAO_0000115>
        <rdfs:label>RandomRotation Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/RandomRotation_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which randomly rotates images during training.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomRotation</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/RandomTranslation_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/RandomTranslation_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A preprocessing layer which randomly translates images during training. This layer will apply random translations to each image during training, filling empty space according to fill_mode. aInput pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats.</obo:IAO_0000115>
        <rdfs:label>RandomTranslation Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/RandomTranslation_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which randomly translates images during training. This layer will apply random translations to each image during training, filling empty space according to fill_mode. aInput pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomTranslation</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/RandomWidth_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/RandomWidth_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A preprocessing layer which randomly varies image width during training. This layer will randomly adjusts the width of a batch of images of a batch of images by a random factor. The input should be a 3D (unbatched) or 4D (batched) tensor in the &quot;channels_last&quot; image data format. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats. By default, this layer is inactive during inference.</obo:IAO_0000115>
        <rdfs:label>RandomWidth Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/RandomWidth_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which randomly varies image width during training. This layer will randomly adjusts the width of a batch of images of a batch of images by a random factor. The input should be a 3D (unbatched) or 4D (batched) tensor in the &quot;channels_last&quot; image data format. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats. By default, this layer is inactive during inference.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomWidth</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/RandomZoom_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/RandomZoom_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A preprocessing layer which randomly zooms images during training. This layer will randomly zoom in or out on each axis of an image independently, filling empty space according to fill_mode.Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats.</obo:IAO_0000115>
        <rdfs:label>RandomZoom Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/RandomZoom_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which randomly zooms images during training. This layer will randomly zoom in or out on each axis of an image independently, filling empty space according to fill_mode.Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomZoom</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Random_Effects_Model -->

    <owl:Class rdf:about="https://w3id.org/aio/Random_Effects_Model">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regression_Analysis"/>
        <obo:IAO_0000115>A statistical model where the model parameters are random variables.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>REM</oboInOwl:hasExactSynonym>
        <rdfs:label>Random Effects Model</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Random_Effects_Model"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A statistical model where the model parameters are random variables.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Random_effects_model</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Random_Forest -->

    <owl:Class rdf:about="https://w3id.org/aio/Random_Forest">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Ensemble_Learning"/>
        <obo:IAO_0000115>An ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time.</obo:IAO_0000115>
        <rdfs:label>Random Forest</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Random_Forest"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>An ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Random_forest</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Ranking_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Ranking_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Anchoring_Bias"/>
        <obo:IAO_0000115>The idea that top-ranked results are the most relevant and important and will result in more clicks than other results.</obo:IAO_0000115>
        <rdfs:label>Ranking Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Ranking_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The idea that top-ranked results are the most relevant and important and will result in more clicks than other results.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Rashomon_Effect_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Rashomon_Effect_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>Refers to differences in perspective, memory and recall, interpretation, and reporting on the same event from multiple persons or witnesses.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Rashomon Effect</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Rashomon Principle</oboInOwl:hasExactSynonym>
        <rdfs:label>Rashomon Effect Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Rashomon_Effect_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Refers to differences in perspective, memory and recall, interpretation, and reporting on the same event from multiple persons or witnesses.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/ReLU_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/ReLU_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>The ReLU activation function returns: max(x, 0), the element-wise maximum of 0 and the input tensor.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>ReLU</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Rectified Linear Unit</oboInOwl:hasExactSynonym>
        <rdfs:label>ReLU Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/ReLU_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The ReLU activation function returns: max(x, 0), the element-wise maximum of 0 and the input tensor.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/ReLU_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/ReLU_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Activation_Layer"/>
        <obo:IAO_0000115>Rectified Linear Unit activation function. With default values, it returns element-wise max(x, 0).</obo:IAO_0000115>
        <rdfs:label>ReLU Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/ReLU_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Rectified Linear Unit activation function. With default values, it returns element-wise max(x, 0).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/RecNN -->

    <owl:Class rdf:about="https://w3id.org/aio/RecNN"/>
    


    <!-- https://w3id.org/aio/Recurrent_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Recurrent_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A layer of an RNB, composed of recurrent units and with the number of which is the hidden size of the layer.</obo:IAO_0000115>
        <rdfs:label>Recurrent Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Recurrent_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A layer of an RNB, composed of recurrent units and with the number of which is the hidden size of the layer.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://docs.nvidia.com/deepLearning/performance/dl-performance-recurrent/index.html#recurrent-layer</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Recurrent_Neural_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Recurrent_Neural_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>RN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>RecNN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Recurrent Network</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Memory Cell, Output</rdfs:comment>
        <rdfs:label>Recurrent Neural Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Recurrent_Neural_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Recurrent_neural_network</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Recursive_Neural_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Recursive_Neural_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>A recursive neural network is a kind of deep neural network created by applying the same set of weights recursively over a structured input, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order. Recursive neural networks, sometimes abbreviated as RvNNs, have been successful, for instance, in Learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>RecuNN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>RvNN</oboInOwl:hasExactSynonym>
        <rdfs:label>Recursive Neural Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Recursive_Neural_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A recursive neural network is a kind of deep neural network created by applying the same set of weights recursively over a structured input, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order. Recursive neural networks, sometimes abbreviated as RvNNs, have been successful, for instance, in Learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Recursive_neural_network</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Regression_Analysis -->

    <owl:Class rdf:about="https://w3id.org/aio/Regression_Analysis">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Supervised_Learning"/>
        <obo:IAO_0000115>A set of statistical processes for estimating the relationships between a dependent variable (often called the &apos;outcome&apos; or &apos;response&apos; variable, or a &apos;label&apos; in machine learning parlance) and one or more independent variables (often called &apos;predictors&apos;, &apos;covariates&apos;, &apos;explanatory variables&apos; or &apos;features&apos;).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Regression analysis</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Regression model</oboInOwl:hasExactSynonym>
        <rdfs:label>Regression Analysis</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Regression_Analysis"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A set of statistical processes for estimating the relationships between a dependent variable (often called the &apos;outcome&apos; or &apos;response&apos; variable, or a &apos;label&apos; in machine learning parlance) and one or more independent variables (often called &apos;predictors&apos;, &apos;covariates&apos;, &apos;explanatory variables&apos; or &apos;features&apos;).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Regression_analysis</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Regularization_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Regularization_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Regularizers allow you to apply penalties on layer parameters or layer activity during optimization. These penalties are summed into the loss function that the network optimizes. Regularization penalties are applied on a per-layer basis.</obo:IAO_0000115>
        <rdfs:label>Regularization Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Regularization_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Regularizers allow you to apply penalties on layer parameters or layer activity during optimization. These penalties are summed into the loss function that the network optimizes. Regularization penalties are applied on a per-layer basis.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://keras.io/api/layers/regularizers/</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Reinforcement_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Reinforcement_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Methods that do not need labelled input/output pairs be presented, nor needing sub-optimal actions to be explicitly corrected. Instead they focus on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).</obo:IAO_0000115>
        <rdfs:label>Reinforcement Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Reinforcement_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that do not need labelled input/output pairs be presented, nor needing sub-optimal actions to be explicitly corrected. Instead they focus on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Reinforcement_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/RepeatVector_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/RepeatVector_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Reshaping_Layer"/>
        <obo:IAO_0000115>Repeats the input n times.</obo:IAO_0000115>
        <rdfs:label>RepeatVector Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/RepeatVector_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Repeats the input n times.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/RepeatVector</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Representation_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Representation_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <obo:IAO_0000115>Arises due to non-random sampling of subgroups, causing trends estimated for one population to not be generalizable to data collected from a new population.</obo:IAO_0000115>
        <rdfs:label>Representation Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Representation_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises due to non-random sampling of subgroups, causing trends estimated for one population to not be generalizable to data collected from a new population.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Representation_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Representation_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>Methods that allow a system to discover the representations required for feature detection or classification from raw data.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Feature Learning</oboInOwl:hasExactSynonym>
        <rdfs:label>Representation Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Representation_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that allow a system to discover the representations required for feature detection or classification from raw data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Feature_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Rescaling_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Rescaling_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Image_Preprocessing_Layer"/>
        <obo:IAO_0000115>A preprocessing layer which rescales input values to a new range.</obo:IAO_0000115>
        <rdfs:label>Rescaling Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Rescaling_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which rescales input values to a new range.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Reshape_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Reshape_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Reshaping_Layer"/>
        <obo:IAO_0000115>Layer that reshapes inputs into the given shape.</obo:IAO_0000115>
        <rdfs:label>Reshape Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Reshape_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer that reshapes inputs into the given shape.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Reshaping_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Reshaping_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Reshape layers are used to change the shape of the input.</obo:IAO_0000115>
        <rdfs:label>Reshaping Layer</rdfs:label>
    </owl:Class>
    


    <!-- https://w3id.org/aio/Residual_Neural_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Residual_Neural_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers. Typical ResNet models are implemented with double- or triple- layer skips that contain nonlinearities (ReLU) and batch normalization in between. An additional weight matrix may be used to learn the skip weights; these models are known as HighwayNets. Models with several parallel skips are referred to as DenseNets. In the context of residual neural networks, a non-residual network may be described as a &apos;plain network&apos;.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>DRN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Deep Residual Network</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>ResNN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>ResNet</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Weight, BN, ReLU, Weight, BN, Addition, ReLU</rdfs:comment>
        <rdfs:label>Residual Neural Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Residual_Neural_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers. Typical ResNet models are implemented with double- or triple- layer skips that contain nonlinearities (ReLU) and batch normalization in between. An additional weight matrix may be used to learn the skip weights; these models are known as HighwayNets. Models with several parallel skips are referred to as DenseNets. In the context of residual neural networks, a non-residual network may be described as a &apos;plain network&apos;.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Residual_neural_network</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Resizing_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Resizing_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Image_Preprocessing_Layer"/>
        <obo:IAO_0000115>A preprocessing layer which resizes images. This layer resizes an image input to a target height and width. The input should be a 4D (batched) or 3D (unbatched) tensor in &quot;channels_last&quot; format. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats. This layer can be called on tf.RaggedTensor batches of input images of distinct sizes, and will resize the outputs to dense tensors of uniform size.</obo:IAO_0000115>
        <rdfs:label>Resizing Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Resizing_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which resizes images. This layer resizes an image input to a target height and width. The input should be a 4D (batched) or 3D (unbatched) tensor in &quot;channels_last&quot; format. Input pixel values can be of any range (e.g. [0., 1.) or [0, 255]) and of interger or floating point dtype. By default, the layer will output floats. This layer can be called on tf.RaggedTensor batches of input images of distinct sizes, and will resize the outputs to dense tensors of uniform size.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Resizing</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Restricted_Boltzmann_Machine -->

    <owl:Class rdf:about="https://w3id.org/aio/Restricted_Boltzmann_Machine">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/BM"/>
        <obo:IAO_0000115>A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>RBM</oboInOwl:hasExactSynonym>
        <rdfs:comment>Backfed Input, Probabilistic Hidden</rdfs:comment>
        <rdfs:label>Restricted Boltzmann Machine</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Restricted_Boltzmann_Machine"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Ridge_Regression -->

    <owl:Class rdf:about="https://w3id.org/aio/Ridge_Regression">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regression_Analysis"/>
        <obo:IAO_0000115>A method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated.[1] It has been used in many fields including econometrics, chemistry, and engineering.</obo:IAO_0000115>
        <rdfs:label>Ridge Regression</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Ridge_Regression"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated.[1] It has been used in many fields including econometrics, chemistry, and engineering.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Ridge_regression</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/SCN -->

    <owl:Class rdf:about="https://w3id.org/aio/SCN"/>
    


    <!-- https://w3id.org/aio/SeLu_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/SeLu_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>The SELU activation function multiplies scale (&gt; 1) with the output of the ELU function to ensure a slope larger than one for positive inputs.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>SELU</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Scaled Exponential Linear Unit</oboInOwl:hasExactSynonym>
        <rdfs:label>SELU Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/SeLu_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The SELU activation function multiplies scale (&gt; 1) with the output of the ELU function to ensure a slope larger than one for positive inputs.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Selection_And_Sampling_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Selection_And_Sampling_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Computational_Bias"/>
        <obo:IAO_0000115>Bias introduced by the selection of individuals, groups, or data for analysis in such a way that proper randomization is not achieved, thereby failing to ensure that the sample obtained is representative of the population intended to be analyzed.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Sampling Bias</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Selection Bias</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Selection Effect</oboInOwl:hasExactSynonym>
        <rdfs:label>Selection And Sampling Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Bias introduced by the selection of individuals, groups, or data for analysis in such a way that proper randomization is not achieved, thereby failing to ensure that the sample obtained is representative of the population intended to be analyzed.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Selection_bias</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Selective_Adherence_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Selective_Adherence_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>Decision-makers’ inclination to selectively adopt algorithmic advice when it matches their pre-existing beliefs and stereotypes.</obo:IAO_0000115>
        <rdfs:label>Selective Adherence Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Selective_Adherence_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Decision-makers’ inclination to selectively adopt algorithmic advice when it matches their pre-existing beliefs and stereotypes.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Self-supervised_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Self-supervised_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Regarded as an intermediate form between supervised and unsupervised learning.</obo:IAO_0000115>
        <rdfs:label>Self-supervised Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Self-supervised_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Regarded as an intermediate form between supervised and unsupervised learning.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Self-supervised_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/SeparableConvolution1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/SeparableConvolution1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Convolutional_Layer"/>
        <obo:IAO_0000115>Depthwise separable 1D convolution. This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output.a</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>SeparableConv1D Layer</oboInOwl:hasExactSynonym>
        <rdfs:label>SeparableConvolution1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/SeparableConvolution1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Depthwise separable 1D convolution. This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output.a</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/SeparableConvolution2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/SeparableConvolution2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Convolutional_Layer"/>
        <obo:IAO_0000115>Depthwise separable 2D convolution. Separable convolutions consist of first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>SeparableConv2D Layer</oboInOwl:hasExactSynonym>
        <rdfs:label>SeparableConvolution2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/SeparableConvolution2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Depthwise separable 2D convolution. Separable convolutions consist of first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Sigmoid_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/Sigmoid_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>Applies the sigmoid activation function sigmoid(x) = 1 / (1 + exp(-x)). For small values (&lt;-5), sigmoid returns a value close to zero, and for large values (&gt;5) the result of the function gets close to 1. Sigmoid is equivalent to a 2-element Softmax, where the second element is assumed to be zero. The sigmoid function always returns a value between 0 and 1.</obo:IAO_0000115>
        <rdfs:label>Sigmoid Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Sigmoid_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies the sigmoid activation function sigmoid(x) = 1 / (1 + exp(-x)). For small values (&lt;-5), sigmoid returns a value close to zero, and for large values (&gt;5) the result of the function gets close to 1. Sigmoid is equivalent to a 2-element Softmax, where the second element is assumed to be zero. The sigmoid function always returns a value between 0 and 1.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/SimpleRNNCell_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/SimpleRNNCell_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Cell class for SimpleRNN. This class processes one step within the whole time sequence input, whereas tf.keras.layer.SimpleRNN processes the whole sequence.</obo:IAO_0000115>
        <rdfs:label>SimpleRNNCell Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/SimpleRNNCell_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Cell class for SimpleRNN. This class processes one step within the whole time sequence input, whereas tf.keras.layer.SimpleRNN processes the whole sequence.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNNCell</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/SimpleRNN_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/SimpleRNN_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Recurrent_Layer"/>
        <obo:IAO_0000115>Fully-connected RNN where the output is to be fed back to input.</obo:IAO_0000115>
        <rdfs:label>SimpleRNN Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/SimpleRNN_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Fully-connected RNN where the output is to be fed back to input.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Societal_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Societal_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Bias"/>
        <obo:IAO_0000115>Can be positive or negative, and take a number of different forms, but is typically characterized as being for or against groups or individuals based on social identities, demographic factors, or immutable physical characteristics. Societal or social biases are often stereotypes. Common examples of societal or social biases are based on concepts like race, ethnicity, gender, sexual orientation, socioeconomic status, education, and more. Societal bias is often recognized and discussed in the context of NLP (Natural Language Processing) models.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Social Bias</oboInOwl:hasExactSynonym>
        <rdfs:label>Societal Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Societal_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Can be positive or negative, and take a number of different forms, but is typically characterized as being for or against groups or individuals based on social identities, demographic factors, or immutable physical characteristics. Societal or social biases are often stereotypes. Common examples of societal or social biases are based on concepts like race, ethnicity, gender, sexual orientation, socioeconomic status, education, and more. Societal bias is often recognized and discussed in the context of NLP (Natural Language Processing) models.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Softmax_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/Softmax_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>The elements of the output vector are in range (0, 1) and sum to 1. Each vector is handled independently. The axis argument sets which axis of the input the function is applied along. Softmax is often used as the activation for the last layer of a classification network because the result could be interpreted as a probability distribution. The softmax of each vector x is computed as exp(x) / tf.reduce_sum(exp(x)). The input values in are the log-odds of the resulting probability.</obo:IAO_0000115>
        <rdfs:label>Softmax Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Softmax_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The elements of the output vector are in range (0, 1) and sum to 1. Each vector is handled independently. The axis argument sets which axis of the input the function is applied along. Softmax is often used as the activation for the last layer of a classification network because the result could be interpreted as a probability distribution. The softmax of each vector x is computed as exp(x) / tf.reduce_sum(exp(x)). The input values in are the log-odds of the resulting probability.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Softmax_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Softmax_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Activation_Layer"/>
        <obo:IAO_0000115>Softmax activation function.</obo:IAO_0000115>
        <rdfs:label>Softmax Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Softmax_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Softmax activation function.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Softmax</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Softplus_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/Softplus_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>softplus(x) = log(exp(x) + 1)</obo:IAO_0000115>
        <rdfs:label>Softplus Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Softplus_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>softplus(x) = log(exp(x) + 1)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/softplus</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Softsign_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/Softsign_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>softsign(x) = x / (abs(x) + 1)</obo:IAO_0000115>
        <rdfs:label>Softsign Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Softsign_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>softsign(x) = x / (abs(x) + 1)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/softsign</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Sparse_AE -->

    <owl:Class rdf:about="https://w3id.org/aio/Sparse_AE">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/AE"/>
        <obo:IAO_0000115>Sparse autoencoders may include more (rather than fewer) hidden units than inputs, but only a small number of the hidden units are allowed to be active at the same time (thus, sparse). This constraint forces the model to respond to the unique statistical features of the training data. (https://en.wikipedia.org/wiki/Autoencoder)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>SAE</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Hidden, Matched Output-Input</rdfs:comment>
        <rdfs:label>Sparse AE</rdfs:label>
    </owl:Class>
    


    <!-- https://w3id.org/aio/Sparse_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Sparse_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Representation_Learning"/>
        <obo:IAO_0000115>Methods which aim to find sparse representations of the input data in the form of a linear combination of basic elements as well as those basic elements themselves.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Sparse coding</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Sparse dictionary Learning</oboInOwl:hasExactSynonym>
        <rdfs:label>Sparse Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Sparse_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods which aim to find sparse representations of the input data in the form of a linear combination of basic elements as well as those basic elements themselves.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Sparse_dictionary_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/SpatialDropout1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/SpatialDropout1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regularization_Layer"/>
        <obo:IAO_0000115>Spatial 1D version of Dropout. This version performs the same function as Dropout, however, it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective Learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.</obo:IAO_0000115>
        <rdfs:label>SpatialDropout1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/SpatialDropout1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Spatial 1D version of Dropout. This version performs the same function as Dropout, however, it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective Learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/SpatialDropout2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/SpatialDropout2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regularization_Layer"/>
        <obo:IAO_0000115>Spatial 2D version of Dropout. This version performs the same function as Dropout, however, it drops entire 2D feature maps instead of individual elements. If adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective Learning rate decrease. In this case, SpatialDropout2D will help promote independence between feature maps and should be used instead.a</obo:IAO_0000115>
        <rdfs:label>SpatialDropout2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/SpatialDropout2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Spatial 2D version of Dropout. This version performs the same function as Dropout, however, it drops entire 2D feature maps instead of individual elements. If adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective Learning rate decrease. In this case, SpatialDropout2D will help promote independence between feature maps and should be used instead.a</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/SpatialDropout3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/SpatialDropout3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regularization_Layer"/>
        <obo:IAO_0000115>Spatial 3D version of Dropout. This version performs the same function as Dropout, however, it drops entire 3D feature maps instead of individual elements. If adjacent voxels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective Learning rate decrease. In this case, SpatialDropout3D will help promote independence between feature maps and should be used instead.</obo:IAO_0000115>
        <rdfs:label>SpatialDropout3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/SpatialDropout3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Spatial 3D version of Dropout. This version performs the same function as Dropout, however, it drops entire 3D feature maps instead of individual elements. If adjacent voxels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective Learning rate decrease. In this case, SpatialDropout3D will help promote independence between feature maps and should be used instead.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout3D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Spatial_Regression -->

    <owl:Class rdf:about="https://w3id.org/aio/Spatial_Regression">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Regression_Analysis"/>
        <obo:IAO_0000115>Regression method used to model spatial relationships.</obo:IAO_0000115>
        <rdfs:label>Spatial Regression</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Spatial_Regression"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Regression method used to model spatial relationships.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://gisgeography.com/spatial-regression-models-arcgis/</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/StackedRNNCells_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/StackedRNNCells_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Wrapper allowing a stack of RNN cells to behave as a single cell. Used to implement efficient stacked RNNs.</obo:IAO_0000115>
        <rdfs:label>StackedRNNCells Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/StackedRNNCells_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Wrapper allowing a stack of RNN cells to behave as a single cell. Used to implement efficient stacked RNNs.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Streetlight_Effect_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Streetlight_Effect_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>A bias whereby people tend to search only where it is easiest to look.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Streetlight Effect</oboInOwl:hasExactSynonym>
        <rdfs:label>Streetlight Effect Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Streetlight_Effect_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A bias whereby people tend to search only where it is easiest to look.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/StringLookup_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/StringLookup_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Categorical_Features_Preprocessing_Layer"/>
        <obo:IAO_0000115>A preprocessing layer which maps string features to integer indices.</obo:IAO_0000115>
        <rdfs:label>StringLookup Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/StringLookup_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which maps string features to integer indices.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Subtract_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Subtract_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Merging_Layer"/>
        <obo:IAO_0000115>Layer that subtracts two inputs. It takes as input a list of tensors of size 2, both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]), also of the same shape.</obo:IAO_0000115>
        <rdfs:label>Subtract Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Subtract_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Layer that subtracts two inputs. It takes as input a list of tensors of size 2, both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]), also of the same shape.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Subtract</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Sunk_Cost_Fallacy_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Sunk_Cost_Fallacy_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Group_Bias"/>
        <obo:IAO_0000115>A human tendency where people opt to continue with an endeavor or behavior due to previously spent or invested resources, such as money, time, and effort, regardless of whether costs outweigh benefits. For example, in AI, the sunk cost fallacy could lead development teams and organizations to feel that because they have already invested so much time and money into a particular AI application, they must pursue it to market rather than deciding to end the effort, even in the face of significant technical debt and/or ethical debt.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Sunk Cost Fallacy</oboInOwl:hasExactSynonym>
        <rdfs:label>Sunk Cost Fallacy Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Sunk_Cost_Fallacy_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A human tendency where people opt to continue with an endeavor or behavior due to previously spent or invested resources, such as money, time, and effort, regardless of whether costs outweigh benefits. For example, in AI, the sunk cost fallacy could lead development teams and organizations to feel that because they have already invested so much time and money into a particular AI application, they must pursue it to market rather than deciding to end the effort, even in the face of significant technical debt and/or ethical debt.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Supervised_Biclustering -->

    <owl:Class rdf:about="https://w3id.org/aio/Supervised_Biclustering">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Biclustering"/>
        <obo:IAO_0000115>Methods that simultaneously cluster the rows and columns of a labeled matrix, also taking into account the data label contributions to cluster coherence.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Supervised Block Clustering</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Supervised Co-clustering</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Supervised Joint Clustering</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Supervised Two-mode Clustering</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Supervised Two-way Clustering</oboInOwl:hasExactSynonym>
        <rdfs:label>Supervised Biclustering</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Supervised_Biclustering"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that simultaneously cluster the rows and columns of a labeled matrix, also taking into account the data label contributions to cluster coherence.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Biclustering</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Supervised_Clustering -->

    <owl:Class rdf:about="https://w3id.org/aio/Supervised_Clustering">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Clustering"/>
        <obo:IAO_0000115>Methods that group a set of labeled objects in such a way that objects in the same group (called a cluster) are more similarly labeled (in some sense) relative to those in other groups (clusters).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Cluster analysis</oboInOwl:hasExactSynonym>
        <rdfs:label>Supervised Clustering</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Supervised_Clustering"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that group a set of labeled objects in such a way that objects in the same group (called a cluster) are more similarly labeled (in some sense) relative to those in other groups (clusters).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Cluster_analysis</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Supervised_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Supervised_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Methods that can learn a function that maps an input to an output based on example input-output pairs.</obo:IAO_0000115>
        <rdfs:label>Supervised Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Supervised_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that can learn a function that maps an input to an output based on example input-output pairs.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Supervised_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Support_Vector_Machine -->

    <owl:Class rdf:about="https://w3id.org/aio/Support_Vector_Machine">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Network"/>
        <obo:IAO_0000115>In machine Learning, support-vector machines (SVMs, also support-vector networks) are supervised Learning models with associated Learning algorithms that analyze data for classification and regression analysis. Developed at AT&amp;T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical Learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>SVM</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>SVN</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Supper Vector Network</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Hidden, Output</rdfs:comment>
        <rdfs:label>Support Vector Machine</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Support_Vector_Machine"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>In machine Learning, support-vector machines (SVMs, also support-vector networks) are supervised Learning models with associated Learning algorithms that analyze data for classification and regression analysis. Developed at AT&amp;T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical Learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Support-vector_machine</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Survival_Analysis -->

    <owl:Class rdf:about="https://w3id.org/aio/Survival_Analysis">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Methods for nalyzing the expected duration of time until one event occurs, such as death in biological organisms and failure in mechanical systems.</obo:IAO_0000115>
        <rdfs:label>Survival Analysis</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Survival_Analysis"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods for nalyzing the expected duration of time until one event occurs, such as death in biological organisms and failure in mechanical systems.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Survival_analysis</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Survivorship_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Survivorship_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Processing_Bias"/>
        <obo:IAO_0000115>Tendency for people to focus on the items, observations, or people that “survive” or make it past a selection process, while overlooking those that did not.</obo:IAO_0000115>
        <rdfs:label>Survivorship Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Survivorship_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Tendency for people to focus on the items, observations, or people that “survive” or make it past a selection process, while overlooking those that did not.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Swish_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/Swish_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>x*sigmoid(x). It is a smooth, non-monotonic function that consistently matches or outperforms ReLU on deep networks, it is unbounded above and bounded below.</obo:IAO_0000115>
        <rdfs:label>Swish Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Swish_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>x*sigmoid(x). It is a smooth, non-monotonic function that consistently matches or outperforms ReLU on deep networks, it is unbounded above and bounded below.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/swish</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Symmetrically_Connected_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Symmetrically_Connected_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Network"/>
        <obo:IAO_0000115>Like recurrent networks, but the connections between units are symmetrical (they have the same weight in both directions).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>SCN</oboInOwl:hasExactSynonym>
        <rdfs:label>Symmetrically Connected Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Symmetrically_Connected_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Like recurrent networks, but the connections between units are symmetrical (they have the same weight in both directions).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://ieeexplore.ieee.org/document/287176</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/SyncBatchNorm_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/SyncBatchNorm_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/BatchNormalization_Layer"/>
        <obo:IAO_0000115>Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>SyncBatchNorm</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>nn.SyncBatchNorm</oboInOwl:hasExactSynonym>
        <rdfs:label>SyncBatchNorm Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/SyncBatchNorm_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://pytorch.org/docs/stable/nn.html#normalization-layers</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Systemic_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Systemic_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Bias"/>
        <obo:IAO_0000115>Biases that result from procedures and practices of particular institutions that operate in ways which result in certain social groups being advantaged or favored and others being disadvantaged or devalued.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Institutional Bias</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Societal Bias</oboInOwl:hasExactSynonym>
        <rdfs:label>Systemic Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Systemic_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Biases that result from procedures and practices of particular institutions that operate in ways which result in certain social groups being advantaged or favored and others being disadvantaged or devalued.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Tanh_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/Tanh_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>Hyperbolic tangent activation function.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>hyperbolic tangent</oboInOwl:hasExactSynonym>
        <rdfs:label>Tanh Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Tanh_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Hyperbolic tangent activation function.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Temporal_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Temporal_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <obo:IAO_0000115>Bias that arises from differences in populations and behaviors over time.</obo:IAO_0000115>
        <rdfs:label>Temporal Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Temporal_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Bias that arises from differences in populations and behaviors over time.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/TextVectorization_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/TextVectorization_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Text_Preprocessing_Layer"/>
        <obo:IAO_0000115>A preprocessing layer which maps text features to integer sequences.</obo:IAO_0000115>
        <rdfs:label>TextVectorization Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/TextVectorization_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A preprocessing layer which maps text features to integer sequences.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Text_Preprocessing_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Text_Preprocessing_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>A layer that performs text data preprocessing operations.</obo:IAO_0000115>
        <rdfs:label>Text Preprocessing Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Text_Preprocessing_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A layer that performs text data preprocessing operations.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://keras.io/guides/preprocessing_layers/</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/ThresholdedReLU_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/ThresholdedReLU_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Activation_Layer"/>
        <obo:IAO_0000115>Thresholded Rectified Linear Unit.</obo:IAO_0000115>
        <rdfs:label>ThresholdedReLU Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/ThresholdedReLU_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Thresholded Rectified Linear Unit.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/ThresholdedReLU</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/TimeDistributed_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/TimeDistributed_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Recurrent_Layer"/>
        <obo:IAO_0000115>This wrapper allows to apply a layer to every temporal slice of an input. Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension. Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with channels_last data format, across 10 timesteps. The batch input shape is (32, 10, 128, 128, 3). You can then use TimeDistributed to apply the same Conv2D layer to each of the 10 timesteps, independently:</obo:IAO_0000115>
        <rdfs:label>TimeDistributed Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/TimeDistributed_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>This wrapper allows to apply a layer to every temporal slice of an input. Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension. Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with channels_last data format, across 10 timesteps. The batch input shape is (32, 10, 128, 128, 3). You can then use TimeDistributed to apply the same Conv2D layer to each of the 10 timesteps, independently:</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Time_Series_Analysis -->

    <owl:Class rdf:about="https://w3id.org/aio/Time_Series_Analysis">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data.</obo:IAO_0000115>
        <rdfs:label>Time Series Analysis</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Time_Series_Analysis"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Time_series</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Time_Series_Forecasting -->

    <owl:Class rdf:about="https://w3id.org/aio/Time_Series_Forecasting">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Methods that predict future values based on previously observed values.</obo:IAO_0000115>
        <rdfs:label>Time Series Forecasting</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Time_Series_Forecasting"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that predict future values based on previously observed values.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Time_series</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Transfer_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Transfer_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Methods which can reuse or transfer information from previously learned tasks for the Learning of new tasks.</obo:IAO_0000115>
        <rdfs:label>Transfer Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Transfer_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods which can reuse or transfer information from previously learned tasks for the Learning of new tasks.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Transfer_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Transformer_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Transformer_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>A transformer is a deep Learning model that adopts the mechanism of attention, differentially weighing the significance of each part of the input data. It is used primarily in the field of natural language processing (NLP) and in computer vision (CV). (https://en.wikipedia.org/wiki/Transformer_(machine_Learning_model))</obo:IAO_0000115>
        <rdfs:label>Transformer Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Transformer_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A transformer is a deep Learning model that adopts the mechanism of attention, differentially weighing the significance of each part of the input data. It is used primarily in the field of natural language processing (NLP) and in computer vision (CV). (https://en.wikipedia.org/wiki/Transformer_(machine_Learning_model))</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Transformer_(machine_Learning_model)</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/UPN -->

    <owl:Class rdf:about="https://w3id.org/aio/UPN"/>
    


    <!-- https://w3id.org/aio/Uncertainty_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Uncertainty_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <obo:IAO_0000115>Arises when predictive algorithms favor groups that are better represented in the training data, since there will be less uncertainty associated with those predictions.</obo:IAO_0000115>
        <rdfs:label>Uncertainty Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Uncertainty_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises when predictive algorithms favor groups that are better represented in the training data, since there will be less uncertainty associated with those predictions.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/UnitNormalization_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/UnitNormalization_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Recurrent_Layer"/>
        <obo:IAO_0000115>Unit normalization layer. Normalize a batch of inputs so that each input in the batch has a L2 norm equal to 1 (across the axes specified in axis).</obo:IAO_0000115>
        <rdfs:label>UnitNormalization Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/UnitNormalization_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Unit normalization layer. Normalize a batch of inputs so that each input in the batch has a L2 norm equal to 1 (across the axes specified in axis).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/UnitNormalization</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Unsupervised_Biclustering -->

    <owl:Class rdf:about="https://w3id.org/aio/Unsupervised_Biclustering">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Biclustering"/>
        <obo:IAO_0000115>Methods that simultaneously cluster the rows and columns of an unlabeled input matrix.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Block Clustering</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Co-clustering</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Joint Clustering</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Two-mode Clustering</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Two-way Clustering</oboInOwl:hasExactSynonym>
        <rdfs:label>Unsupervised Biclustering</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Unsupervised_Biclustering"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that simultaneously cluster the rows and columns of an unlabeled input matrix.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Biclustering</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Unsupervised_Clustering -->

    <owl:Class rdf:about="https://w3id.org/aio/Unsupervised_Clustering">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Clustering"/>
        <obo:IAO_0000115>Methods that group a set of objects in such a way that objects without labels in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Cluster analysis</oboInOwl:hasExactSynonym>
        <rdfs:label>Unsupervised Clustering</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Unsupervised_Clustering"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods that group a set of objects in such a way that objects without labels in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Cluster_analysis</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Unsupervised_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Unsupervised_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Machine_Learning"/>
        <obo:IAO_0000115>Algorithms that learns patterns from unlabeled data.</obo:IAO_0000115>
        <rdfs:label>Unsupervised Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Unsupervised_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Algorithms that learns patterns from unlabeled data.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Unsupervised_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Unsupervised_Pretrained_Network -->

    <owl:Class rdf:about="https://w3id.org/aio/Unsupervised_Pretrained_Network">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Network"/>
        <obo:IAO_0000115>Unsupervised pre-training initializes a discriminative neural net from one which was trained using an unsupervised criterion, such as a deep belief network or a deep autoencoder. This method can sometimes help with both the optimization and the overfitting issues.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>UPN</oboInOwl:hasExactSynonym>
        <rdfs:label>Unsupervised Pretrained Network</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Unsupervised_Pretrained_Network"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Unsupervised pre-training initializes a discriminative neural net from one which was trained using an unsupervised criterion, such as a deep belief network or a deep autoencoder. This method can sometimes help with both the optimization and the overfitting issues.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://metacademy.org/graphs/concepts/unsupervised_pre_training#:~:text=Unsupervised%20pre%2Dtraining%20initializes%20a,optimization%20and%20the%20overfitting%20issues</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/UpSampling1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/UpSampling1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Reshaping_Layer"/>
        <obo:IAO_0000115>Upsampling layer for 1D inputs. Repeats each temporal step size times along the time axis.</obo:IAO_0000115>
        <rdfs:label>UpSampling1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/UpSampling1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Upsampling layer for 1D inputs. Repeats each temporal step size times along the time axis.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/UpSampling2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/UpSampling2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Upsampling layer for 2D inputs. Repeats the rows and columns of the data by size[0] and size[1] respectively.</obo:IAO_0000115>
        <rdfs:label>UpSampling2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/UpSampling2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Upsampling layer for 2D inputs. Repeats the rows and columns of the data by size[0] and size[1] respectively.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/UpSampling3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/UpSampling3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Upsampling layer for 3D inputs.</obo:IAO_0000115>
        <rdfs:label>UpSampling3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/UpSampling3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Upsampling layer for 3D inputs.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling3D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Use_And_Interpretation_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Use_And_Interpretation_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Computational_Bias"/>
        <obo:IAO_0000115>An information-processing bias, the tendency to inappropriately analyze ambiguous stimuli, scenarios and events.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Interpretive Bias</oboInOwl:hasExactSynonym>
        <rdfs:label>Use And Interpretation Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Use_And_Interpretation_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>An information-processing bias, the tendency to inappropriately analyze ambiguous stimuli, scenarios and events.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Interpretive_bias</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/User_Interaction_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/User_Interaction_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Individual_Bias"/>
        <obo:IAO_0000115>Arises when a user imposes their own self-selected biases and behavior during interaction with data, output, results, etc.</obo:IAO_0000115>
        <rdfs:label>User Interaction Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/User_Interaction_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Arises when a user imposes their own self-selected biases and behavior during interaction with data, output, results, etc.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Variational_Auto_Encoder -->

    <owl:Class rdf:about="https://w3id.org/aio/Variational_Auto_Encoder">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/AE"/>
        <obo:IAO_0000115>Variational autoencoders are meant to compress the input information into a constrained multivariate latent distribution (encoding) to reconstruct it as accurately as possible (decoding). (https://en.wikipedia.org/wiki/Variational_autoencoder)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>VAE</oboInOwl:hasExactSynonym>
        <rdfs:comment>Input, Probabilistic Hidden, Matched Output-Input</rdfs:comment>
        <rdfs:label>Variational Auto Encoder</rdfs:label>
    </owl:Class>
    


    <!-- https://w3id.org/aio/W2V_CBOW -->

    <owl:Class rdf:about="https://w3id.org/aio/W2V_CBOW"/>
    


    <!-- https://w3id.org/aio/W2V_SkipGram -->

    <owl:Class rdf:about="https://w3id.org/aio/W2V_SkipGram"/>
    


    <!-- https://w3id.org/aio/Wrapper_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/Wrapper_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Layer"/>
        <obo:IAO_0000115>Abstract wrapper base class. Wrappers take another layer and augment it in various ways. Do not use this class as a layer, it is only an abstract base class. Two usable wrappers are the TimeDistributed and Bidirectional wrappers.</obo:IAO_0000115>
        <rdfs:label>Wrapper Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Wrapper_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Abstract wrapper base class. Wrappers take another layer and augment it in various ways. Do not use this class as a layer, it is only an abstract base class. Two usable wrappers are the TimeDistributed and Bidirectional wrappers.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Wrapper</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Zero-shot_Learning -->

    <owl:Class rdf:about="https://w3id.org/aio/Zero-shot_Learning">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/DNN"/>
        <obo:IAO_0000115>Methods where at test time, a learner observes samples from classes, which were not observed during training, and needs to predict the class that they belong to.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>ZSL</oboInOwl:hasExactSynonym>
        <rdfs:label>Zero-shot Learning</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Zero-shot_Learning"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Methods where at test time, a learner observes samples from classes, which were not observed during training, and needs to predict the class that they belong to.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Zero-shot_learning</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/ZeroPadding1D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/ZeroPadding1D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Reshaping_Layer"/>
        <obo:IAO_0000115>Zero-padding layer for 1D input (e.g. temporal sequence).</obo:IAO_0000115>
        <rdfs:label>ZeroPadding1D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/ZeroPadding1D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Zero-padding layer for 1D input (e.g. temporal sequence).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/ZeroPadding1D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/ZeroPadding2D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/ZeroPadding2D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Reshaping_Layer"/>
        <obo:IAO_0000115>Zero-padding layer for 2D input (e.g. picture). This layer can add rows and columns of zeros at the top, bottom, left and right side of an image tensor.</obo:IAO_0000115>
        <rdfs:label>ZeroPadding2D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/ZeroPadding2D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Zero-padding layer for 2D input (e.g. picture). This layer can add rows and columns of zeros at the top, bottom, left and right side of an image tensor.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/ZeroPadding2D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/ZeroPadding3D_Layer -->

    <owl:Class rdf:about="https://w3id.org/aio/ZeroPadding3D_Layer">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Reshaping_Layer"/>
        <obo:IAO_0000115>Zero-padding layer for 3D data (spatial or spatio-temporal).</obo:IAO_0000115>
        <rdfs:label>ZeroPadding3D Layer</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/ZeroPadding3D_Layer"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>Zero-padding layer for 3D data (spatial or spatio-temporal).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/layers/ZeroPadding3D</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/eLu_Function -->

    <owl:Class rdf:about="https://w3id.org/aio/eLu_Function">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Function"/>
        <obo:IAO_0000115>The exponential linear unit (ELU) with alpha &gt; 0 is: x if x &gt; 0 and alpha * (exp(x) - 1) if x &lt; 0 The ELU hyperparameter alpha controls the value to which an ELU saturates for negative net inputs. ELUs diminish the vanishing gradient effect. ELUs have negative values which pushes the mean of the activations closer to zero. Mean activations that are closer to zero enable faster Learning as they bring the gradient closer to the natural gradient. ELUs saturate to a negative value when the argument gets smaller. Saturation means a small derivative which decreases the variation and the information that is propagated to the next layer.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>ELU</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>Exponential Linear Unit</oboInOwl:hasExactSynonym>
        <rdfs:label>ELU Function</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/eLu_Function"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>The exponential linear unit (ELU) with alpha &gt; 0 is: x if x &gt; 0 and alpha * (exp(x) - 1) if x &lt; 0 The ELU hyperparameter alpha controls the value to which an ELU saturates for negative net inputs. ELUs diminish the vanishing gradient effect. ELUs have negative values which pushes the mean of the activations closer to zero. Mean activations that are closer to zero enable faster Learning as they bring the gradient closer to the natural gradient. ELUs saturate to a negative value when the argument gets smaller. Saturation means a small derivative which decreases the variation and the information that is propagated to the next layer.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/node2vec-CBOW -->

    <owl:Class rdf:about="https://w3id.org/aio/node2vec-CBOW">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/W2V_CBOW"/>
        <obo:IAO_0000115>In the continuous bag-of-words architecture, the model predicts the current node from a window of surrounding context nodes. The order of context nodes does not influence prediction (bag-of-words assumption).</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>N2V-CBOW</oboInOwl:hasExactSynonym>
        <oboInOwl:hasRelatedSynonym>CBOW</oboInOwl:hasRelatedSynonym>
        <rdfs:comment>Input, Hidden, Output</rdfs:comment>
        <rdfs:label>node2vec-CBOW</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/node2vec-CBOW"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>In the continuous bag-of-words architecture, the model predicts the current node from a window of surrounding context nodes. The order of context nodes does not influence prediction (bag-of-words assumption).</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Word2vec</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/node2vec-SkipGram -->

    <owl:Class rdf:about="https://w3id.org/aio/node2vec-SkipGram">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/W2V_SkipGram"/>
        <obo:IAO_0000115>In the continuous skip-gram architecture, the model uses the current node to predict the surrounding window of context nodes. The skip-gram architecture weighs nearby context nodes more heavily than more distant context nodes. (https://en.wikipedia.org/wiki/Word2vec)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>N2V-SkipGram</oboInOwl:hasExactSynonym>
        <oboInOwl:hasRelatedSynonym>SkipGram</oboInOwl:hasRelatedSynonym>
        <rdfs:comment>Input, Hidden, Output</rdfs:comment>
        <rdfs:label>node2vec-SkipGram</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/node2vec-SkipGram"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>In the continuous skip-gram architecture, the model uses the current node to predict the surrounding window of context nodes. The skip-gram architecture weighs nearby context nodes more heavily than more distant context nodes. (https://en.wikipedia.org/wiki/Word2vec)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Word2vec</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/t-Distributed_Stochastic_Neighbor_embedding -->

    <owl:Class rdf:about="https://w3id.org/aio/t-Distributed_Stochastic_Neighbor_embedding">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Dimensionality_Reduction"/>
        <obo:IAO_0000115>A statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>t-SNE</oboInOwl:hasExactSynonym>
        <oboInOwl:hasExactSynonym>tSNE</oboInOwl:hasExactSynonym>
        <rdfs:label>t-Distributed Stochastic Neighbor embedding</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/t-Distributed_Stochastic_Neighbor_embedding"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/word2vec-CBOW -->

    <owl:Class rdf:about="https://w3id.org/aio/word2vec-CBOW">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/ANN"/>
        <obo:IAO_0000115>In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). (https://en.wikipedia.org/wiki/Word2vec)</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>W2V-CBOW</oboInOwl:hasExactSynonym>
        <oboInOwl:hasRelatedSynonym>CBOW</oboInOwl:hasRelatedSynonym>
        <rdfs:comment>Input, Hidden, Output</rdfs:comment>
        <rdfs:label>word2vec-CBOW</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/word2vec-CBOW"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). (https://en.wikipedia.org/wiki/Word2vec)</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Word2vec</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/word2vec-SkipGram -->

    <owl:Class rdf:about="https://w3id.org/aio/word2vec-SkipGram">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/ANN"/>
        <obo:IAO_0000115>In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weighs nearby context words more heavily than more distant context words.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>W2V-SkipGram</oboInOwl:hasExactSynonym>
        <oboInOwl:hasRelatedSynonym>SkipGram</oboInOwl:hasRelatedSynonym>
        <rdfs:comment>Input, Hidden, Output</rdfs:comment>
        <rdfs:label>word2vec-SkipGram</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/word2vec-SkipGram"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weighs nearby context words more heavily than more distant context words.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://en.wikipedia.org/wiki/Word2vec</oboInOwl:hasDbXref>
    </owl:Axiom>
    


    <!-- https://w3id.org/aio/Simpon&apos;s_Paradox_Bias -->

    <owl:Class rdf:about="https://w3id.org/aio/Simpon&apos;s_Paradox_Bias">
        <rdfs:subClassOf rdf:resource="https://w3id.org/aio/Selection_And_Sampling_Bias"/>
        <obo:IAO_0000115>A statistical phenomenon where the marginal association between two categorical variables is qualitatively different from the partial association between the same two variables after controlling for one or more other variables. For example, the statistical association or correlation that has been detected between two variables for an entire population disappears or reverses when the population is divided into subgroups.</obo:IAO_0000115>
        <oboInOwl:hasExactSynonym>Simpson&apos;s Paradox</oboInOwl:hasExactSynonym>
        <rdfs:label>Simpon&apos;s Paradox Bias</rdfs:label>
    </owl:Class>
    <owl:Axiom>
        <owl:annotatedSource rdf:resource="https://w3id.org/aio/Simpon&apos;s_Paradox_Bias"/>
        <owl:annotatedProperty rdf:resource="http://purl.obolibrary.org/obo/IAO_0000115"/>
        <owl:annotatedTarget>A statistical phenomenon where the marginal association between two categorical variables is qualitatively different from the partial association between the same two variables after controlling for one or more other variables. For example, the statistical association or correlation that has been detected between two variables for an entire population disappears or reverses when the population is divided into subgroups.</owl:annotatedTarget>
        <oboInOwl:hasDbXref>https://doi.org/10.6028/NIST.SP.1270</oboInOwl:hasDbXref>
    </owl:Axiom>
</rdf:RDF>



<!-- Generated by the OWL API (version 4.5.25) https://github.com/owlcs/owlapi -->

